{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import soxr\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Define key landmark indices for emotion-relevant features\n",
    "LEFT_EYE = [33, 133, 160, 159, 158, 144, 153, 154, 155, 173, 157, 163]\n",
    "RIGHT_EYE = [362, 385, 387, 388, 466, 263, 249, 390, 373, 374, 380, 381]\n",
    "LEFT_EYEBROW = [70, 63, 105, 66, 107]\n",
    "RIGHT_EYEBROW = [336, 296, 334, 293, 300]\n",
    "MOUTH_OUTER = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146]\n",
    "MOUTH_INNER = [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 324, 318, 402, 317, 14, 87, 178, 88, 95]\n",
    "NOSE = [1, 2, 3, 4, 5, 6, 168, 197, 195, 5, 4, 98, 97, 2, 326, 327]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=8)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=8)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=8)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv4 = nn.Conv1d(128, 64, kernel_size=3, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_classes=8, dropout_rate=0.3):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        if lstm_out.size(1) == 1:\n",
    "            lstm_out = lstm_out.squeeze(1)\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        lstm_out = self.bn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_EMOTIONS = 8 \n",
    "EMOTION_LABELS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "class VotingEnsemble(nn.Module):\n",
    "    def __init__(self, speech_model, facial_model):\n",
    "        super(VotingEnsemble, self).__init__()\n",
    "        self.speech_model = speech_model\n",
    "        self.facial_model = facial_model\n",
    "        \n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.speech_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        self.facial_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "    def forward(self, speech_input, facial_input):\n",
    "        speech_out = self.speech_model(speech_input)\n",
    "        facial_out = self.facial_model(facial_input)\n",
    "        \n",
    "        speech_probs = torch.softmax(speech_out, dim=1) * torch.sigmoid(self.speech_weight)\n",
    "        facial_probs = torch.softmax(facial_out, dim=1) * torch.sigmoid(self.facial_weight)\n",
    "        weighted_avg = (speech_probs + facial_probs) / (torch.sigmoid(self.speech_weight) + torch.sigmoid(self.facial_weight))\n",
    "        \n",
    "        return weighted_avg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, speech_data, facial_data, labels, device='cuda'):\n",
    "    model.eval()\n",
    "    \n",
    "    speech_data = speech_data.to(device)\n",
    "    facial_data = facial_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(speech_data, facial_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        predicted_np = predicted.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        acc = accuracy_score(labels_np, predicted_np)\n",
    "        report = classification_report(labels_np, predicted_np, target_names=EMOTION_LABELS, output_dict=True)\n",
    "        conf_matrix = confusion_matrix(labels_np, predicted_np)\n",
    "        \n",
    "        per_class_acc = report['weighted avg']['precision']\n",
    "        \n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'per_class_accuracy': per_class_acc,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "\n",
    "def visualize_confusion_matrix(conf_matrix, class_names=EMOTION_LABELS):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_individual_vs_ensemble(speech_model, facial_model, ensemble_model, \n",
    "                                   speech_data, facial_data,device='cuda'):\n",
    "    speech_data = speech_data.to(device)\n",
    "    facial_data = facial_data.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    \n",
    "    speech_model.eval()\n",
    "    facial_model.eval()\n",
    "    ensemble_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        speech_outputs = speech_model(speech_data)\n",
    "        facial_outputs = facial_model(facial_data)\n",
    "        ensemble_outputs = ensemble_model(speech_data, facial_data)\n",
    "\n",
    "        # return speech_outputs, facial_outputs, ensemble_outputs\n",
    "        \n",
    "        _, speech_preds = torch.max(speech_outputs, 1)\n",
    "        _, facial_preds = torch.max(facial_outputs, 1)\n",
    "        _, ensemble_preds = torch.max(ensemble_outputs, 1)\n",
    "        \n",
    "        speech_preds_np = speech_preds.cpu().numpy()\n",
    "        facial_preds_np = facial_preds.cpu().numpy()\n",
    "        ensemble_preds_np = ensemble_preds.cpu().numpy()\n",
    "\n",
    "        return speech_preds_np, facial_preds_np, ensemble_preds_np\n",
    "        # labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        # speech_acc = accuracy_score(labels_np, speech_preds_np)\n",
    "        # facial_acc = accuracy_score(labels_np, facial_preds_np)\n",
    "        # ensemble_acc = accuracy_score(labels_np, ensemble_preds_np)\n",
    "        \n",
    "        # speech_report = classification_report(labels_np, speech_preds_np, \n",
    "        #                                      target_names=EMOTION_LABELS, output_dict=True)\n",
    "        # facial_report = classification_report(labels_np, facial_preds_np, \n",
    "        #                                      target_names=EMOTION_LABELS, output_dict=True)\n",
    "        # ensemble_report = classification_report(labels_np, ensemble_preds_np, \n",
    "        #                                       target_names=EMOTION_LABELS, output_dict=True)\n",
    "        \n",
    "        # speech_cm = confusion_matrix(labels_np, speech_preds_np)\n",
    "        # facial_cm = confusion_matrix(labels_np, facial_preds_np)\n",
    "        # ensemble_cm = confusion_matrix(labels_np, ensemble_preds_np)\n",
    "        \n",
    "        # return {\n",
    "        #     'speech': {\n",
    "        #         'accuracy': speech_acc,\n",
    "        #         'classification_report': speech_report,\n",
    "        #         'confusion_matrix': speech_cm\n",
    "        #     },\n",
    "        #     'facial': {\n",
    "        #         'accuracy': facial_acc,\n",
    "        #         'classification_report': facial_report,\n",
    "        #         'confusion_matrix': facial_cm\n",
    "        #     },\n",
    "        #     'ensemble': {\n",
    "        #         'accuracy': ensemble_acc,\n",
    "        #         'classification_report': ensemble_report,\n",
    "        #         'confusion_matrix': ensemble_cm\n",
    "        #     }\n",
    "        # }\n",
    "\n",
    "def plot_comparison(results):\n",
    "    models = ['Speech Model', 'Facial Model', 'Ensemble Model']\n",
    "    accuracies = [\n",
    "        results['speech']['accuracy'] * 100,\n",
    "        results['facial']['accuracy'] * 100,\n",
    "        results['ensemble']['accuracy'] * 100\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Model Comparison')\n",
    "    plt.ylim(0, 100)  \n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_audio(video_path):\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    audio_clip = video_clip.audio\n",
    "    # audio_path = f\"C:/Nini/Capstone/Test/{video_path.split('/')[5].split('.')[0]}.wav\"\n",
    "    audio_path = \"dummy.wav\"\n",
    "    audio_clip.write_audiofile(audio_path)\n",
    "\n",
    "    X, sample_rate = librosa.load(audio_path,res_type='kaiser_fast',duration=3,sr=44100,offset=0.5)\n",
    "    audio_resampled = soxr.resample(X, sample_rate, 16000)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=audio_resampled,sr=16000,n_mels=128,fmax=8000)\n",
    "    db_spec = librosa.power_to_db(spectrogram)\n",
    "    log_spectrogram = np.mean(db_spec,axis=1)\n",
    "\n",
    "    mean = np.load(\"C:/Nini/Capstone/src/Data Preprocessing/mean.npy\")\n",
    "    std = np.load(\"C:/Nini/Capstone/src/Data Preprocessing/std.npy\")\n",
    "\n",
    "    mean_tensor = torch.from_numpy(mean).float()\n",
    "    std_tensor = torch.from_numpy(std).float()\n",
    "    log_spectrogram = torch.from_numpy(log_spectrogram).float()\n",
    "    log_spectrogram = (log_spectrogram - mean_tensor) / std_tensor\n",
    "\n",
    "    log_spectrogram = log_spectrogram.unsqueeze(0).unsqueeze(1).float()\n",
    "    return log_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(p1, p2):\n",
    "    return np.linalg.norm(p1 - p2)\n",
    "\n",
    "def calculate_eye_aspect_ratio(eye):\n",
    "    v1 = calculate_distance(eye[1], eye[7])\n",
    "    v2 = calculate_distance(eye[2], eye[6])\n",
    "    v3 = calculate_distance(eye[3], eye[5])\n",
    "    h = calculate_distance(eye[0], eye[4])\n",
    "    return (v1 + v2 + v3) / (3.0 * h)\n",
    "\n",
    "def calculate_mouth_aspect_ratio(outer, inner):\n",
    "    outer_v = calculate_distance(outer[3], outer[9])\n",
    "    inner_v = calculate_distance(inner[3], inner[9])\n",
    "    h = calculate_distance(outer[0], outer[6])\n",
    "    return outer_v / h, inner_v / outer_v\n",
    "\n",
    "def calculate_eyebrow_position(eyebrow, eye):\n",
    "    return np.mean([p[1] for p in eye]) - np.mean([p[1] for p in eyebrow])\n",
    "\n",
    "def extract_features_from_frame(frame):\n",
    "    left_eye = frame[LEFT_EYE]\n",
    "    right_eye = frame[RIGHT_EYE]\n",
    "    left_eyebrow = frame[LEFT_EYEBROW]\n",
    "    right_eyebrow = frame[RIGHT_EYEBROW]\n",
    "    mouth_outer = frame[MOUTH_OUTER]\n",
    "    mouth_inner = frame[MOUTH_INNER]\n",
    "    nose = frame[NOSE]\n",
    "\n",
    "    width = np.max(frame[:, 0]) - np.min(frame[:, 0])\n",
    "    height = np.max(frame[:, 1]) - np.min(frame[:, 1])\n",
    "\n",
    "    left_ear = calculate_eye_aspect_ratio(left_eye)\n",
    "    right_ear = calculate_eye_aspect_ratio(right_eye)\n",
    "    mar, openness = calculate_mouth_aspect_ratio(mouth_outer, mouth_inner)\n",
    "    l_eyebrow_pos = calculate_eyebrow_position(left_eyebrow, left_eye) / height\n",
    "    r_eyebrow_pos = calculate_eyebrow_position(right_eyebrow, right_eye) / height\n",
    "    mouth_center_y = (mouth_outer[3][1] + mouth_outer[9][1]) / 2\n",
    "    smile = ((mouth_center_y - mouth_outer[0][1]) + (mouth_center_y - mouth_outer[6][1])) / (2 * height)\n",
    "    nose_wrinkle = np.std([p[2] for p in nose])\n",
    "    eye_sym = abs(left_ear - right_ear)\n",
    "    brow_sym = abs(l_eyebrow_pos - r_eyebrow_pos)\n",
    "\n",
    "    return [left_ear, right_ear, mar, openness, l_eyebrow_pos, r_eyebrow_pos, smile, nose_wrinkle, eye_sym, brow_sym]\n",
    "\n",
    "def summarize_video_features(features):\n",
    "    features = np.array(features)\n",
    "    summary = []\n",
    "    for i in range(features.shape[1]):\n",
    "        f = features[:, i]\n",
    "        summary.extend([np.mean(f), np.std(f), np.min(f), np.max(f), np.max(f)-np.min(f), f[-1] - f[0]])\n",
    "    return summary\n",
    "\n",
    "def extract_landmarks_from_video(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    features = []\n",
    "    with mp_face_mesh.FaceMesh(\n",
    "        static_image_mode=False,\n",
    "        max_num_faces=1,\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5) as fm:\n",
    "        while cap.isOpened():\n",
    "            ret, img = cap.read()\n",
    "            if not ret: break\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            result = fm.process(img_rgb)\n",
    "            if result.multi_face_landmarks:\n",
    "                lm = result.multi_face_landmarks[0]\n",
    "                points = np.array([[p.x, p.y, p.z] for p in lm.landmark])\n",
    "                try:\n",
    "                    features.append(extract_features_from_frame(points))\n",
    "                except:\n",
    "                    continue\n",
    "    cap.release()\n",
    "    return summarize_video_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_face(video_path):\n",
    "    df = []\n",
    "    landmarks = extract_landmarks_from_video(video_path)\n",
    "    df.append(landmarks)\n",
    "    columns = [\n",
    "            'left_eye_ar', 'right_eye_ar', 'mouth_ar', 'mouth_openness',\n",
    "            'left_eyebrow_pos', 'right_eyebrow_pos', 'smile_ratio', 'nose_wrinkle',\n",
    "            'eye_symmetry', 'eyebrow_symmetry'\n",
    "        ]\n",
    "    stats = ['mean', 'std', 'min', 'max', 'range', 'delta']\n",
    "    colnames = [f\"{f}_{s}\" for f in columns for s in stats]\n",
    "    feats = pd.DataFrame(df,columns=colnames)\n",
    "\n",
    "    face_features = feats.values\n",
    "    with open(\"C:/Nini/Capstone/CSV_Files/Facial data/New Facial Data/robust_scaler-2.pkl\", 'rb') as r:\n",
    "        scaler = pickle.load(r)\n",
    "    face_features_scaled = scaler.transform(face_features)\n",
    "    tensor_face = torch.tensor(face_features_scaled, dtype=torch.float32).reshape((face_features_scaled.shape[0], 1, face_features_scaled.shape[1]))\n",
    "    return tensor_face, face_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_found': True, 'audio_found': True, 'metadata': {'major_brand': 'isom', 'minor_version': '512', 'compatible_brands': 'isomiso2avc1mp41', 'encoder': 'Lavf61.7.100'}, 'inputs': [{'streams': [{'input_number': 0, 'stream_number': 0, 'stream_type': 'video', 'language': None, 'default': True, 'size': [480, 360], 'bitrate': 233, 'fps': 29.97002997002997, 'codec_name': 'h264', 'profile': '(High)', 'metadata': {'Metadata': '', 'handler_name': 'VideoHandler', 'vendor_id': '[0][0][0][0]', 'encoder': 'Lavc61.19.100 libx264'}}, {'input_number': 0, 'stream_number': 1, 'stream_type': 'audio', 'language': None, 'default': True, 'fps': 44100, 'bitrate': 130, 'metadata': {'Metadata': '', 'handler_name': 'SoundHandler', 'vendor_id': '[0][0][0][0]'}}], 'input_number': 0}], 'duration': 2.5, 'bitrate': 375, 'start': 0.0, 'default_video_input_number': 0, 'default_video_stream_number': 0, 'video_codec_name': 'h264', 'video_profile': '(High)', 'video_size': [480, 360], 'video_bitrate': 233, 'video_fps': 29.97002997002997, 'default_audio_input_number': 0, 'default_audio_stream_number': 1, 'audio_fps': 44100, 'audio_bitrate': 130, 'video_duration': 2.5, 'video_n_frames': 74}\n",
      "c:\\Nini\\Capstone\\src\\capstone_env\\Lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -i C:/Nini/Capstone/Test/1001_TSI_ANG_XX.mp4 -loglevel error -f image2pipe -vf scale=480:360 -sws_flags bicubic -pix_fmt rgb24 -vcodec rawvideo -\n",
      "MoviePy - Writing audio in dummy.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "The class predicted by the speech model is: ['angry']\n",
      "The class predicted by the facial model is: ['disgust']\n",
      "The class predicted by the ensemble model is: ['disgust']\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # df_speech = pd.read_csv(\"C:/Nini/Capstone/CSV_Files/Speech data/Speech_Test1_preprocessed_new.csv\")\n",
    "    # X_speech = df_speech.drop(columns=['Path','Unnamed: 0','Emotion']).values \n",
    "    # X_tensor_speech = torch.tensor(X_speech, dtype=torch.float32).unsqueeze(1)\n",
    "    # torch.save(X_tensor_speech, \"C:/Nini/Capstone/CSV_Files/Speech data/speech_test_data.pt\")\n",
    "    # speech_test_data = torch.load(\"C:/Nini/Capstone/CSV_Files/Speech data/speech_test_data.pt\", weights_only=True)\n",
    "\n",
    "    # df_face = pd.read_excel(\"C:/Nini/Capstone/CSV_Files/Facial data/New Facial Data/FaceTest(e)(arranged).xlsx\")\n",
    "    # X_face = df_face.drop(columns=['video_path','emotion']).values\n",
    "    # with open(\"C:/Nini/Capstone/CSV_Files/Facial data/New Facial Data/robust_scaler-2.pkl\", 'rb') as r:\n",
    "    #     scaler = pickle.load(r)\n",
    "    # X_face_scaled = scaler.fit_transform(X_face)\n",
    "    # X_tensor_face = torch.tensor(X_face_scaled, dtype=torch.float32).reshape((X_face_scaled.shape[0], 1, X_face_scaled.shape[1]))\n",
    "    # torch.save(X_tensor_face, 'C:/Nini/Capstone/CSV_Files/Facial data/New Facial Data/face_test_data.pt')\n",
    "    # facial_test_data = torch.load('C:/Nini/Capstone/CSV_Files/Facial data/New Facial Data/face_test_data.pt', weights_only=True)\n",
    "\n",
    "    # with open(\"C:/Nini/Capstone/src/Model_training/label_encoder.pkl\", 'rb') as f:\n",
    "    #     label_encoder = pickle.load(f)\n",
    "    # test_labels = df_face['emotion'].values\n",
    "    # test_labels_encoded = label_encoder.transform(test_labels)\n",
    "    # test_labels_tensor = torch.tensor(test_labels_encoded, dtype=torch.long)\n",
    "    # torch.save(test_labels_tensor, \"C:/Nini/Capstone/CSV_Files/Speech data/test_labels.pt\")\n",
    "    # test_labels = torch.load(\"C:/Nini/Capstone/CSV_Files/Speech data/test_labels.pt\", weights_only=True)\n",
    "    video_path = \"C:/Nini/Capstone/Test/1001_TSI_ANG_XX.mp4\"\n",
    "    speech_features = extract_features_from_audio(video_path)\n",
    "    face_features, X_face = extract_features_from_face(video_path)\n",
    "    \n",
    "    \n",
    "    speech_model = CNNModel(input_size=speech_features.shape[1], num_classes=8).to(device)\n",
    "    facial_model = EmotionLSTM(input_size=X_face.shape[1]).to(device)\n",
    "    \n",
    "    speech_model.load_state_dict(torch.load('C:/Nini/Capstone/Models/DataAugmentation_cnn_model_new_final_1.pth', weights_only=True))\n",
    "    facial_model.load_state_dict(torch.load('C:/Nini/Capstone/Models/emotion_lstm_model-7.pth', weights_only=True))\n",
    "    \n",
    "    ensemble_model = VotingEnsemble(speech_model, facial_model).to(device)\n",
    "    \n",
    "    speech_results, facial_results, ensemble_results = evaluate_individual_vs_ensemble(\n",
    "        speech_model, facial_model, ensemble_model,\n",
    "        speech_features, face_features, device\n",
    "    )\n",
    "    with open(\"C:/Nini/Capstone/src/Model_training/label_encoder.pkl\", 'rb') as f:\n",
    "        label_encoder = pickle.load(f)\n",
    "    \n",
    "    speech_emotion = label_encoder.inverse_transform(speech_results)\n",
    "    facial_emotion = label_encoder.inverse_transform(facial_results)\n",
    "    ensemble_emotion = label_encoder.inverse_transform(ensemble_results)\n",
    "    print(f'The class predicted by the speech model is: {speech_emotion}')\n",
    "    print(f'The class predicted by the facial model is: {facial_emotion}')\n",
    "    print(f'The class predicted by the ensemble model is: {ensemble_emotion}')\n",
    "\n",
    "    \n",
    "\n",
    "    # print(f\"Speech Model Accuracy: {results['speech']['accuracy']:.4f}\")\n",
    "    # print(f\"Facial Model Accuracy: {results['facial']['accuracy']:.4f}\")\n",
    "    # print(f\"Ensemble Model Accuracy: {results['ensemble']['accuracy']:.4f}\")\n",
    "\n",
    "    # plot_comparison(results)\n",
    "    \n",
    "    # visualize_confusion_matrix(results['ensemble']['confusion_matrix'])\n",
    "    \n",
    "    # print(\"\\nEnsemble Classification Report:\")\n",
    "    # for emotion, metrics in results['ensemble']['classification_report'].items():\n",
    "    #     if isinstance(metrics, dict):\n",
    "    #         print(f\"{emotion}: precision={metrics['precision']:.2f}, recall={metrics['recall']:.2f}, f1-score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7125212,
     "sourceId": 11379962,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7164187,
     "sourceId": 11437306,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7166468,
     "sourceId": 11440262,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7169219,
     "sourceId": 11444025,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 306323,
     "modelInstanceId": 285494,
     "sourceId": 341307,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 306599,
     "modelInstanceId": 285763,
     "sourceId": 341628,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 306613,
     "modelInstanceId": 285776,
     "sourceId": 341646,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
