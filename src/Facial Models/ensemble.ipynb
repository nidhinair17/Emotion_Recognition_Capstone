{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=8)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=8)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=8)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv4 = nn.Conv1d(128, 64, kernel_size=3, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=192, num_classes=8, dropout_rate=0.3):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "        # Simplified architecture - one bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Simple dense layers\n",
    "        self.bn = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Get last time step or squeeze\n",
    "        if lstm_out.size(1) == 1:\n",
    "            lstm_out = lstm_out.squeeze(1)\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# speech_features = pd.read_csv('path/to/speech_features.csv')\n",
    "\n",
    "# # Load facial features (8 Excel files for different emotions)\n",
    "# emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "# facial_features_by_emotion = {}\n",
    "\n",
    "# for emotion in emotions:\n",
    "#     facial_features_by_emotion[emotion] = pd.read_excel(f'path/to/facial_features_{emotion}.xlsx')\n",
    "\n",
    "# # Now we need to combine these into appropriate tensors\n",
    "# # This depends on your exact data format, but here's a general approach:\n",
    "\n",
    "# # 1. Extract features and labels from your data\n",
    "# X_speech = speech_features.drop('E', axis=1).values  # Assuming there's a label column\n",
    "# y = speech_features['emotion_label'].values  # Assuming labels are in the speech CSV\n",
    "\n",
    "# # 2. Combine facial features (this depends on your exact format)\n",
    "# # Method 1: If each Excel has the same samples in the same order\n",
    "# # X_facial = []\n",
    "# # for i, sample in enumerate(X_speech):\n",
    "# #     # Get the emotion label for this sample\n",
    "# #     emotion = emotions[y[i]]\n",
    "# #     # Get the corresponding facial features\n",
    "# #     facial_sample = facial_features_by_emotion[emotion].iloc[i].values\n",
    "# #     X_facial.append(facial_sample)\n",
    "# # X_facial = np.array(X_facial)\n",
    "\n",
    "# # Alternative Method 2: If Excel files are organized by emotion and have some ID column\n",
    "# # This assumes each file only has data for one emotion and has an ID column to match with speech data\n",
    "# X_facial = np.zeros((len(X_speech), facial_features_dimensionality))\n",
    "# for i, row in speech_features.iterrows():\n",
    "#     sample_id = row['Emotion']  # Assuming there's an ID column\n",
    "#     emotion = emotions[row['emotion_label']]\n",
    "#     # Find the matching facial features using the ID\n",
    "#     matching_facial = facial_features_by_emotion[emotion][\n",
    "#         facial_features_by_emotion[emotion]['Emotion'] == sample_id]\n",
    "#     if not matching_facial.empty:\n",
    "#         X_facial[i] = matching_facial.drop('Emotion', axis=1).values[0]\n",
    "\n",
    "# # 3. Create train/test split (or train/val/test if training the ensemble)\n",
    "# X_speech_train, X_speech_test, X_facial_train, X_facial_test, y_train, y_test = train_test_split(\n",
    "#     X_speech, X_facial, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NUM_EMOTIONS = 8  # For RAVDESS dataset: neutral, calm, happy, sad, angry, fearful, disgust, surprised\n",
    "\n",
    "# # Voting Ensemble Model\n",
    "# class VotingEnsemble(nn.Module):\n",
    "#     def __init__(self, speech_model, facial_model):\n",
    "#         super(VotingEnsemble, self).__init__()\n",
    "#         self.speech_model = speech_model\n",
    "#         self.facial_model = facial_model\n",
    "        \n",
    "#         # Freeze base models (optional - you can set to True if you want to keep original models fixed)\n",
    "#         for param in self.speech_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.facial_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "            \n",
    "#         # Learnable weights for each model\n",
    "#         self.speech_weight = nn.Parameter(torch.tensor(0.5))\n",
    "#         self.facial_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "#         # Additional fusion layer for feature-level combination\n",
    "#         self.fusion_layer = nn.Linear(NUM_EMOTIONS * 2, NUM_EMOTIONS)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "#     def forward(self, speech_input, facial_input):\n",
    "#         # Get predictions from individual models\n",
    "#         speech_out = self.speech_model(speech_input)\n",
    "#         facial_out = self.facial_model(facial_input)\n",
    "        \n",
    "#         # Method 1: Weighted average of probabilities\n",
    "#         speech_probs = torch.softmax(speech_out, dim=1) * torch.sigmoid(self.speech_weight)\n",
    "#         facial_probs = torch.softmax(facial_out, dim=1) * torch.sigmoid(self.facial_weight)\n",
    "#         weighted_avg = (speech_probs + facial_probs) / (torch.sigmoid(self.speech_weight) + torch.sigmoid(self.facial_weight))\n",
    "        \n",
    "#         # Method 2: Feature-level fusion\n",
    "#         concat_features = torch.cat((speech_out, facial_out), dim=1)\n",
    "#         fused_output = self.fusion_layer(self.dropout(concat_features))\n",
    "        \n",
    "#         # You can choose which method to use based on performance\n",
    "#         # return weighted_avg  # Method 1\n",
    "#         return fused_output  # Method 2\n",
    "\n",
    "# # Alternative: Simple majority voting ensemble (no training required)\n",
    "# class MajorityVotingEnsemble(nn.Module):\n",
    "#     def __init__(self, speech_model, facial_model):\n",
    "#         super(MajorityVotingEnsemble, self).__init__()\n",
    "#         self.speech_model = speech_model\n",
    "#         self.facial_model = facial_model\n",
    "        \n",
    "#         # Freeze base models\n",
    "#         for param in self.speech_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.facial_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "    \n",
    "#     def forward(self, speech_input, facial_input):\n",
    "#         # Get predictions from individual models\n",
    "#         speech_out = self.speech_model(speech_input)\n",
    "#         facial_out = self.facial_model(facial_input)\n",
    "        \n",
    "#         # Simple average of probabilities\n",
    "#         speech_probs = torch.softmax(speech_out, dim=1)\n",
    "#         facial_probs = torch.softmax(facial_out, dim=1)\n",
    "        \n",
    "#         return (speech_probs + facial_probs) / 2\n",
    "\n",
    "# # Boosting-inspired ensemble that learns to correct errors\n",
    "# class BoostingEnsemble(nn.Module):\n",
    "#     def __init__(self, speech_model, facial_model):\n",
    "#         super(BoostingEnsemble, self).__init__()\n",
    "#         self.speech_model = speech_model\n",
    "#         self.facial_model = facial_model\n",
    "        \n",
    "#         # Freeze base models\n",
    "#         for param in self.speech_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.facial_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         # Sequential combination - first model feeds into correction network\n",
    "#         hidden_size = 128\n",
    "#         self.error_correction = nn.Sequential(\n",
    "#             nn.Linear(NUM_EMOTIONS * 2, hidden_size),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden_size, NUM_EMOTIONS)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, speech_input, facial_input):\n",
    "#         # Get base predictions\n",
    "#         speech_logits = self.speech_model(speech_input)\n",
    "#         facial_logits = self.facial_model(facial_input)\n",
    "        \n",
    "#         # Combine logits for error correction\n",
    "#         combined = torch.cat((speech_logits, facial_logits), dim=1)\n",
    "        \n",
    "#         # Apply correction network\n",
    "#         final_output = self.error_correction(combined)\n",
    "        \n",
    "#         return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_EMOTIONS = 8  # For RAVDESS dataset: neutral, calm, happy, sad, angry, fearful, disgust, surprised\n",
    "EMOTION_LABELS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "# Voting Ensemble Model\n",
    "class VotingEnsemble(nn.Module):\n",
    "    def __init__(self, speech_model, facial_model):\n",
    "        super(VotingEnsemble, self).__init__()\n",
    "        self.speech_model = speech_model\n",
    "        self.facial_model = facial_model\n",
    "        \n",
    "        # Freeze base models (optional - you can set to True if you want to keep original models fixed)\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Learnable weights for each model\n",
    "        self.speech_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        self.facial_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        # Additional fusion layer for feature-level combination\n",
    "        self.fusion_layer = nn.Linear(NUM_EMOTIONS * 2, NUM_EMOTIONS)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, speech_input, facial_input):\n",
    "        # Get predictions from individual models\n",
    "        speech_out = self.speech_model(speech_input)\n",
    "        facial_out = self.facial_model(facial_input)\n",
    "        \n",
    "        # Method 1: Weighted average of probabilities\n",
    "        speech_probs = torch.softmax(speech_out, dim=1) * torch.sigmoid(self.speech_weight)\n",
    "        facial_probs = torch.softmax(facial_out, dim=1) * torch.sigmoid(self.facial_weight)\n",
    "        weighted_avg = (speech_probs + facial_probs) / (torch.sigmoid(self.speech_weight) + torch.sigmoid(self.facial_weight))\n",
    "        \n",
    "        # Method 2: Feature-level fusion\n",
    "        concat_features = torch.cat((speech_out, facial_out), dim=1)\n",
    "        fused_output = self.fusion_layer(self.dropout(concat_features))\n",
    "        \n",
    "        # You can choose which method to use based on performance\n",
    "        # return weighted_avg  # Method 1\n",
    "        return fused_output  # Method 2\n",
    "\n",
    "# Alternative: Simple majority voting ensemble (no training required)\n",
    "class MajorityVotingEnsemble(nn.Module):\n",
    "    def __init__(self, speech_model, facial_model):\n",
    "        super(MajorityVotingEnsemble, self).__init__()\n",
    "        self.speech_model = speech_model\n",
    "        self.facial_model = facial_model\n",
    "        \n",
    "        # Freeze base models\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, speech_input, facial_input):\n",
    "        # Get predictions from individual models\n",
    "        speech_out = self.speech_model(speech_input)\n",
    "        facial_out = self.facial_model(facial_input)\n",
    "        \n",
    "        # Simple average of probabilities\n",
    "        speech_probs = torch.softmax(speech_out, dim=1)\n",
    "        facial_probs = torch.softmax(facial_out, dim=1)\n",
    "        \n",
    "        return (speech_probs + facial_probs) / 2\n",
    "\n",
    "# Boosting-inspired ensemble that learns to correct errors\n",
    "class BoostingEnsemble(nn.Module):\n",
    "    def __init__(self, speech_model, facial_model):\n",
    "        super(BoostingEnsemble, self).__init__()\n",
    "        self.speech_model = speech_model\n",
    "        self.facial_model = facial_model\n",
    "        \n",
    "        # Freeze base models\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Sequential combination - first model feeds into correction network\n",
    "        hidden_size = 128\n",
    "        self.error_correction = nn.Sequential(\n",
    "            nn.Linear(NUM_EMOTIONS * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, NUM_EMOTIONS)\n",
    "        )\n",
    "    \n",
    "    def forward(self, speech_input, facial_input):\n",
    "        # Get base predictions\n",
    "        speech_logits = self.speech_model(speech_input)\n",
    "        facial_logits = self.facial_model(facial_input)\n",
    "        \n",
    "        # Combine logits for error correction\n",
    "        combined = torch.cat((speech_logits, facial_logits), dim=1)\n",
    "        \n",
    "        # Apply correction network\n",
    "        final_output = self.error_correction(combined)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate_model(model, speech_data, facial_data, labels, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate the ensemble model and return metrics\n",
    "    \n",
    "    Args:\n",
    "        model: The ensemble model\n",
    "        speech_data: Speech features (tensor)\n",
    "        facial_data: Facial features (tensor)\n",
    "        labels: Ground truth labels (tensor)\n",
    "        device: Device to run evaluation on ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Move data to device\n",
    "    speech_data = speech_data.to(device)\n",
    "    facial_data = facial_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(speech_data, facial_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        predicted_np = predicted.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(labels_np, predicted_np)\n",
    "        report = classification_report(labels_np, predicted_np, target_names=EMOTION_LABELS, output_dict=True)\n",
    "        conf_matrix = confusion_matrix(labels_np, predicted_np)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_acc = report['weighted avg']['precision']\n",
    "        \n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'per_class_accuracy': per_class_acc,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "\n",
    "def visualize_confusion_matrix(conf_matrix, class_names=EMOTION_LABELS):\n",
    "    \"\"\"\n",
    "    Visualize confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        conf_matrix: Confusion matrix\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_individual_vs_ensemble(speech_model, facial_model, ensemble_model, \n",
    "                                   speech_data, facial_data, labels, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compare individual models vs ensemble\n",
    "    \n",
    "    Args:\n",
    "        speech_model: Speech emotion model\n",
    "        facial_model: Facial emotion model\n",
    "        ensemble_model: Ensemble model\n",
    "        speech_data: Speech features (tensor)\n",
    "        facial_data: Facial features (tensor)\n",
    "        labels: Ground truth labels (tensor)\n",
    "        device: Device to run evaluation on ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with all evaluation results\n",
    "    \"\"\"\n",
    "    # Move data to device\n",
    "    speech_data = speech_data.to(device)\n",
    "    facial_data = facial_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    speech_model.eval()\n",
    "    facial_model.eval()\n",
    "    ensemble_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Individual model predictions\n",
    "        speech_outputs = speech_model(speech_data)\n",
    "        facial_outputs = facial_model(facial_data)\n",
    "        ensemble_outputs = ensemble_model(speech_data, facial_data)\n",
    "        \n",
    "        # Get predicted classes\n",
    "        _, speech_preds = torch.max(speech_outputs, 1)\n",
    "        _, facial_preds = torch.max(facial_outputs, 1)\n",
    "        _, ensemble_preds = torch.max(ensemble_outputs, 1)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        speech_preds_np = speech_preds.cpu().numpy()\n",
    "        facial_preds_np = facial_preds.cpu().numpy()\n",
    "        ensemble_preds_np = ensemble_preds.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        speech_acc = accuracy_score(labels_np, speech_preds_np)\n",
    "        facial_acc = accuracy_score(labels_np, facial_preds_np)\n",
    "        ensemble_acc = accuracy_score(labels_np, ensemble_preds_np)\n",
    "        \n",
    "        # Generate classification reports\n",
    "        speech_report = classification_report(labels_np, speech_preds_np, \n",
    "                                             target_names=EMOTION_LABELS, output_dict=True)\n",
    "        facial_report = classification_report(labels_np, facial_preds_np, \n",
    "                                             target_names=EMOTION_LABELS, output_dict=True)\n",
    "        ensemble_report = classification_report(labels_np, ensemble_preds_np, \n",
    "                                              target_names=EMOTION_LABELS, output_dict=True)\n",
    "        \n",
    "        # Generate confusion matrices\n",
    "        speech_cm = confusion_matrix(labels_np, speech_preds_np)\n",
    "        facial_cm = confusion_matrix(labels_np, facial_preds_np)\n",
    "        ensemble_cm = confusion_matrix(labels_np, ensemble_preds_np)\n",
    "        \n",
    "        # Return all results\n",
    "        return {\n",
    "            'speech': {\n",
    "                'accuracy': speech_acc,\n",
    "                'classification_report': speech_report,\n",
    "                'confusion_matrix': speech_cm\n",
    "            },\n",
    "            'facial': {\n",
    "                'accuracy': facial_acc,\n",
    "                'classification_report': facial_report,\n",
    "                'confusion_matrix': facial_cm\n",
    "            },\n",
    "            'ensemble': {\n",
    "                'accuracy': ensemble_acc,\n",
    "                'classification_report': ensemble_report,\n",
    "                'confusion_matrix': ensemble_cm\n",
    "            }\n",
    "        }\n",
    "\n",
    "def plot_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot comparison of models\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from evaluate_individual_vs_ensemble\n",
    "    \"\"\"\n",
    "    models = ['Speech Model', 'Facial Model', 'Ensemble Model']\n",
    "    accuracies = [\n",
    "        results['speech']['accuracy'] * 100,\n",
    "        results['facial']['accuracy'] * 100,\n",
    "        results['ensemble']['accuracy'] * 100\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Model Comparison')\n",
    "    plt.ylim(0, 100)  # Set y-axis from 0 to 100\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Load your models and data here\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Initialize your models\n",
    "    speech_model = CNNModel(input_size=torch.Size([1, 128]), num_classes=8).to(device)\n",
    "    facial_model = EmotionLSTM(input_size=21).to(device)\n",
    "    \n",
    "    # Load pre-trained weights\n",
    "    speech_model.load_state_dict(torch.load('C:/Nini/Capstone/Models/DataAugmentation_cnn_model_new.pth', weights_only=True))\n",
    "    facial_model.load_state_dict(torch.load('/kaggle/input/face-emotion/pytorch/default/1/emotion_lstm_model-2.pth', weights_only=True))\n",
    "    \n",
    "    # Create ensemble model\n",
    "    ensemble_model = VotingEnsemble(speech_model, facial_model).to(device)\n",
    "    \n",
    "    # Load test data\n",
    "    speech_test_data = torch.load('path/to/speech_test_data.pt')\n",
    "    facial_test_data = torch.load('path/to/facial_test_data.pt')\n",
    "    test_labels = torch.load('path/to/test_labels.pt')\n",
    "    \n",
    "    # Evaluate models\n",
    "    results = evaluate_individual_vs_ensemble(\n",
    "        speech_model, facial_model, ensemble_model,\n",
    "        speech_test_data, facial_test_data, test_labels, device\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Speech Model Accuracy: {results['speech']['accuracy']:.4f}\")\n",
    "    print(f\"Facial Model Accuracy: {results['facial']['accuracy']:.4f}\")\n",
    "    print(f\"Ensemble Model Accuracy: {results['ensemble']['accuracy']:.4f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_comparison(results)\n",
    "    \n",
    "    # Visualize confusion matrix for the ensemble\n",
    "    visualize_confusion_matrix(results['ensemble']['confusion_matrix'])\n",
    "    \n",
    "    # Print detailed report for ensemble\n",
    "    print(\"\\nEnsemble Classification Report:\")\n",
    "    for emotion, metrics in results['ensemble']['classification_report'].items():\n",
    "        if isinstance(metrics, dict):\n",
    "            print(f\"{emotion}: precision={metrics['precision']:.2f}, recall={metrics['recall']:.2f}, f1-score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Initialize your pre-trained models\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# speech_model = CNNModel(input_size=torch.Size([1, 128]), num_classes=8).to(device)\n",
    "\n",
    "# facial_model = EmotionLSTM(input_size=21)\n",
    "\n",
    "# # Load your pre-trained weights\n",
    "# speech_model.load_state_dict(torch.load('/kaggle/input/speech-emotion-detection/pytorch/default/1/DataAugmentation_cnn_model_new.pth', weights_only=True))\n",
    "# facial_model.load_state_dict(torch.load('/kaggle/input/face-emotion/pytorch/default/1/emotion_lstm_model-2.pth', weights_only=True))\n",
    "\n",
    "# # Create ensemble\n",
    "# ensemble = VotingEnsemble(speech_model, facial_model)  # Or any other ensemble variant\n",
    "\n",
    "# # Use the ensemble for prediction\n",
    "# # speech_features = torch.FloatTensor(your_speech_features)\n",
    "# # facial_features = torch.FloatTensor(your_facial_features)\n",
    "# # predictions = ensemble(speech_features, facial_features)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "modelId": 290776,
     "modelInstanceId": 269786,
     "sourceId": 319733,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 294839,
     "modelInstanceId": 273943,
     "sourceId": 325946,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
