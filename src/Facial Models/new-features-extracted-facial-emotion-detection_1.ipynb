{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:01.158630Z",
     "iopub.status.busy": "2025-04-17T06:40:01.158332Z",
     "iopub.status.idle": "2025-04-17T06:40:10.291227Z",
     "shell.execute_reply": "2025-04-17T06:40:10.290221Z",
     "shell.execute_reply.started": "2025-04-17T06:40:01.158607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.6.1\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.5->scikit-learn==1.6.1) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.6.1) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.5->scikit-learn==1.6.1) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.5->scikit-learn==1.6.1) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.5->scikit-learn==1.6.1) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.5->scikit-learn==1.6.1) (2024.2.0)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed scikit-learn-1.6.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:10.293362Z",
     "iopub.status.busy": "2025-04-17T06:40:10.293082Z",
     "iopub.status.idle": "2025-04-17T06:40:19.509962Z",
     "shell.execute_reply": "2025-04-17T06:40:19.508981Z",
     "shell.execute_reply.started": "2025-04-17T06:40:10.293337Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:19.511356Z",
     "iopub.status.busy": "2025-04-17T06:40:19.510818Z",
     "iopub.status.idle": "2025-04-17T06:40:19.517616Z",
     "shell.execute_reply": "2025-04-17T06:40:19.516777Z",
     "shell.execute_reply.started": "2025-04-17T06:40:19.511332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:19.519236Z",
     "iopub.status.busy": "2025-04-17T06:40:19.518916Z",
     "iopub.status.idle": "2025-04-17T06:40:19.542064Z",
     "shell.execute_reply": "2025-04-17T06:40:19.541257Z",
     "shell.execute_reply.started": "2025-04-17T06:40:19.519211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def load_emotion_data(train_path, test_path):\n",
    "#     try:\n",
    "#         train_df = pd.read_excel(train_path)\n",
    "#         test_df = pd.read_excel(test_path)\n",
    "        \n",
    "#         print(f\"Loaded training data: {len(train_df)} samples\")\n",
    "#         print(f\"Loaded testing data: {len(test_df)} samples\")\n",
    "        \n",
    "#         feature_columns = ['left_eye_ar', 'right_eye_ar', 'mouth_ar',\t\n",
    "#                            'mouth_openness', 'left_eyebrow_pos', 'right_eyebrow_pos',\t\n",
    "#                            'smile_ratio', 'nose_wrinkle', 'eye_symmetry', 'eyebrow_symmetry']\n",
    "        \n",
    "#         label_column = 'emotion'\n",
    "        \n",
    "#         # Extract features and labels\n",
    "#         X_train = train_df[feature_columns].values\n",
    "#         X_test = test_df[feature_columns].values\n",
    "        \n",
    "#         # Encode labels\n",
    "#         with open('/kaggle/input/label-encoderr/label_encoder (1).pkl','rb') as file:\n",
    "#             label_encoder = pickle.load(file)\n",
    "#         y_train = label_encoder.transform(train_df[label_column])\n",
    "#         y_test = label_encoder.transform(test_df[label_column])\n",
    "        \n",
    "#         print(\"\\nDataset Statistics:\")\n",
    "#         print(f\"Total training samples: {len(train_df)}\")\n",
    "#         print(f\"Total testing samples: {len(test_df)}\")\n",
    "#         print(\"\\nTraining samples per emotion:\")\n",
    "#         print(train_df[label_column].value_counts())\n",
    "#         print(\"\\nTesting samples per emotion:\")\n",
    "#         print(test_df[label_column].value_counts())\n",
    "        \n",
    "#         return X_train, X_test, y_train, y_test, label_encoder\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading data: {str(e)}\")\n",
    "#         raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:19.544114Z",
     "iopub.status.busy": "2025-04-17T06:40:19.543812Z",
     "iopub.status.idle": "2025-04-17T06:40:19.569377Z",
     "shell.execute_reply": "2025-04-17T06:40:19.568417Z",
     "shell.execute_reply.started": "2025-04-17T06:40:19.544093Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_emotion_data(train_path, test_path):\n",
    "    try:\n",
    "        train_df = pd.read_excel(train_path)\n",
    "        test_df = pd.read_excel(test_path)\n",
    "        \n",
    "        print(f\"Loaded training data: {len(train_df)} samples\")\n",
    "        print(f\"Loaded testing data: {len(test_df)} samples\")\n",
    "        \n",
    "        # feature_columns = ['left_eye_ar', 'right_eye_ar', 'mouth_ar',\t\n",
    "        #                    'mouth_openness', 'left_eyebrow_pos', 'right_eyebrow_pos',\t\n",
    "        #                    'smile_ratio', 'nose_wrinkle', 'eye_symmetry', 'eyebrow_symmetry']\n",
    "        columns = [\n",
    "        'left_eye_ar', 'right_eye_ar', 'mouth_ar', 'mouth_openness',\n",
    "        'left_eyebrow_pos', 'right_eyebrow_pos', 'smile_ratio', 'nose_wrinkle',\n",
    "        'eye_symmetry', 'eyebrow_symmetry'\n",
    "        ]\n",
    "        stats = ['mean', 'std', 'min', 'max', 'range', 'delta']\n",
    "        feature_columns = [f\"{f}_{s}\" for f in columns for s in stats]\n",
    "        label_column = 'emotion'\n",
    "        \n",
    "        # Extract features and labels\n",
    "        X_train = train_df[feature_columns].values\n",
    "        X_test = test_df[feature_columns].values\n",
    "        \n",
    "        # Encode labels\n",
    "        with open('/kaggle/input/label-encoderr/label_encoder (1).pkl','rb') as file:\n",
    "            label_encoder = pickle.load(file)\n",
    "        y_train = label_encoder.transform(train_df[label_column])\n",
    "        y_test = label_encoder.transform(test_df[label_column])\n",
    "        \n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        print(f\"Total training samples: {len(train_df)}\")\n",
    "        print(f\"Total testing samples: {len(test_df)}\")\n",
    "        print(\"\\nTraining samples per emotion:\")\n",
    "        print(train_df[label_column].value_counts())\n",
    "        print(\"\\nTesting samples per emotion:\")\n",
    "        print(test_df[label_column].value_counts())\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, label_encoder\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:19.571151Z",
     "iopub.status.busy": "2025-04-17T06:40:19.570495Z",
     "iopub.status.idle": "2025-04-17T06:40:19.591636Z",
     "shell.execute_reply": "2025-04-17T06:40:19.590551Z",
     "shell.execute_reply.started": "2025-04-17T06:40:19.571124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=128, num_classes=8, dropout_rate=0.3):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "        # Simplified architecture - one bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Simple dense layers\n",
    "        self.bn = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Get last time step or squeeze\n",
    "        if lstm_out.size(1) == 1:\n",
    "            lstm_out = lstm_out.squeeze(1)\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:19.593338Z",
     "iopub.status.busy": "2025-04-17T06:40:19.593059Z",
     "iopub.status.idle": "2025-04-17T06:40:19.623238Z",
     "shell.execute_reply": "2025-04-17T06:40:19.622316Z",
     "shell.execute_reply.started": "2025-04-17T06:40:19.593309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionDetectionPyTorch:\n",
    "    def __init__(self, X_train, X_test, y_train, y_test, label_encoder):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        # Standardize features\n",
    "        self.scaler = RobustScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        joblib.dump(self.scaler, \"robust_scaler.pkl\")\n",
    "        print(\"Robust Scaler saved successfully!\")\n",
    "        with open('/kaggle/working/robust_scaler.pkl', 'wb') as file:\n",
    "            pickle.dump(self.scaler, file)\n",
    "        \n",
    "        # Reshape for LSTM (samples, sequence_length, features)\n",
    "        self.X_train_reshaped = self.X_train_scaled.reshape(\n",
    "            (self.X_train_scaled.shape[0], 1, self.X_train_scaled.shape[1]))\n",
    "        self.X_test_reshaped = self.X_test_scaled.reshape(\n",
    "            (self.X_test_scaled.shape[0], 1, self.X_test_scaled.shape[1]))\n",
    "        \n",
    "        self.label_encoder = label_encoder\n",
    "        self.num_classes = len(label_encoder.classes_)\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        # Create datasets and dataloaders\n",
    "        self.train_dataset = EmotionDataset(self.X_train_reshaped, self.y_train)\n",
    "        self.test_dataset = EmotionDataset(self.X_test_reshaped, self.y_test)\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        print(f\"\\nTraining set shape: {self.X_train.shape}\")\n",
    "        print(f\"Testing set shape: {self.X_test.shape}\")\n",
    "\n",
    "    def create_dataloaders(self, batch_size=32):\n",
    "        \"\"\"Create DataLoaders for train and test sets\"\"\"\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def create_lstm_model(self):\n",
    "        \"\"\"Create LSTM model\"\"\"\n",
    "        input_dim = self.X_train.shape[1]  # Number of features\n",
    "        hidden_dim = 128\n",
    "        model = EmotionLSTM(input_size=self.X_train.shape[1])\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, train_loader, test_loader, epochs=1000, learning_rate=0.0005):\n",
    "        \"\"\"Train the PyTorch model\"\"\"\n",
    "\n",
    "        # Create loss function with weights\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # For tracking metrics\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        \n",
    "        print(f\"\\nTraining LSTM model...\")\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            epoch_acc = correct / total\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accs.append(epoch_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_running_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "            val_epoch_acc = val_correct / val_total\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            val_accs.append(val_epoch_acc)\n",
    "            \n",
    "            # Print statistics\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - \"\n",
    "                      f\"Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}\")\n",
    "        \n",
    "        history = {\n",
    "            'loss': train_losses,\n",
    "            'accuracy': train_accs,\n",
    "            'val_loss': val_losses,\n",
    "            'val_accuracy': val_accs\n",
    "        }\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def evaluate_model(self, model, test_loader):\n",
    "        \"\"\"Evaluate the model on test data\"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Convert indices to emotion labels\n",
    "        pred_emotions = self.label_encoder.inverse_transform(all_preds)\n",
    "        true_emotions = self.label_encoder.inverse_transform(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        conf_matrix = confusion_matrix(true_emotions, pred_emotions)\n",
    "        class_report = classification_report(true_emotions, pred_emotions)\n",
    "        \n",
    "        return conf_matrix, class_report, pred_emotions, true_emotions\n",
    "    \n",
    "    def plot_confusion_matrix(self, conf_matrix, title, emotion_labels):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "        plt.title(title)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_training_history(self, history, title):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{title} - Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Training Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{title} - Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def run_experiment(self, model_name=\"LSTM\", batch_size=32, epochs=1500, learning_rate=0.0005):\n",
    "        \"\"\"Run a complete experiment\"\"\"\n",
    "        # Create dataloaders\n",
    "        train_loader, test_loader = self.create_dataloaders(batch_size)\n",
    "        \n",
    "        # Create model\n",
    "        model = self.create_lstm_model()\n",
    "        \n",
    "        # Train model\n",
    "        trained_model, history = self.train_model(\n",
    "            model, train_loader, test_loader, epochs, learning_rate\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        conf_matrix, class_report, pred_emotions, true_emotions = self.evaluate_model(\n",
    "            trained_model, test_loader\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'model': trained_model,\n",
    "            'history': history,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report,\n",
    "            'predictions': pred_emotions,\n",
    "            'true_labels': true_emotions\n",
    "        }\n",
    "        \n",
    "        # Plot results\n",
    "        self.plot_training_history(history, model_name)\n",
    "        self.plot_confusion_matrix(\n",
    "            conf_matrix, f'{model_name} Confusion Matrix', \n",
    "            self.label_encoder.classes_\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{model_name} Classification Report:\")\n",
    "        print(class_report)\n",
    "        \n",
    "        return trained_model\n",
    "\n",
    "    def save_model(self, model, path='/kaggle/working/emotion_lstm_model.pth'):\n",
    "        \"\"\"Save the PyTorch model\"\"\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path='/kaggle/working/emotion_lstm_model.pth'):\n",
    "        \"\"\"Load a saved PyTorch model\"\"\"\n",
    "        model = self.create_lstm_model()\n",
    "        model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T06:40:19.624443Z",
     "iopub.status.busy": "2025-04-17T06:40:19.624080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data: 2315 samples\n",
      "Loaded testing data: 508 samples\n",
      "\n",
      "Dataset Statistics:\n",
      "Total training samples: 2315\n",
      "Total testing samples: 508\n",
      "\n",
      "Training samples per emotion:\n",
      "emotion\n",
      "angry        320\n",
      "fearful      313\n",
      "disgust      313\n",
      "surprised    309\n",
      "sad          307\n",
      "calm         307\n",
      "happy        294\n",
      "neutral      152\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Testing samples per emotion:\n",
      "emotion\n",
      "happy        81\n",
      "calm         69\n",
      "sad          69\n",
      "surprised    67\n",
      "disgust      64\n",
      "fearful      64\n",
      "angry        58\n",
      "neutral      36\n",
      "Name: count, dtype: int64\n",
      "Robust Scaler saved successfully!\n",
      "Using device: cpu\n",
      "\n",
      "Training set shape: (2315, 60)\n",
      "Testing set shape: (508, 60)\n",
      "\n",
      "Training LSTM model...\n",
      "Epoch 1/1500 - Loss: 1.7098 - Acc: 0.3741 - Val Loss: 1.3581 - Val Acc: 0.5394\n",
      "Epoch 5/1500 - Loss: 1.0147 - Acc: 0.6354 - Val Loss: 0.9618 - Val Acc: 0.6496\n",
      "Epoch 10/1500 - Loss: 0.7884 - Acc: 0.7162 - Val Loss: 0.7647 - Val Acc: 0.7205\n",
      "Epoch 15/1500 - Loss: 0.6615 - Acc: 0.7581 - Val Loss: 0.6465 - Val Acc: 0.7697\n",
      "Epoch 20/1500 - Loss: 0.5732 - Acc: 0.7896 - Val Loss: 0.5587 - Val Acc: 0.8091\n",
      "Epoch 25/1500 - Loss: 0.4541 - Acc: 0.8371 - Val Loss: 0.5246 - Val Acc: 0.8248\n",
      "Epoch 30/1500 - Loss: 0.4012 - Acc: 0.8639 - Val Loss: 0.4653 - Val Acc: 0.8406\n",
      "Epoch 35/1500 - Loss: 0.3588 - Acc: 0.8717 - Val Loss: 0.4453 - Val Acc: 0.8563\n",
      "Epoch 40/1500 - Loss: 0.3020 - Acc: 0.8955 - Val Loss: 0.4031 - Val Acc: 0.8642\n",
      "Epoch 45/1500 - Loss: 0.3041 - Acc: 0.8929 - Val Loss: 0.4226 - Val Acc: 0.8543\n",
      "Epoch 50/1500 - Loss: 0.2646 - Acc: 0.9067 - Val Loss: 0.3962 - Val Acc: 0.8701\n",
      "Epoch 55/1500 - Loss: 0.2394 - Acc: 0.9227 - Val Loss: 0.3645 - Val Acc: 0.8878\n",
      "Epoch 60/1500 - Loss: 0.1987 - Acc: 0.9317 - Val Loss: 0.3985 - Val Acc: 0.8858\n",
      "Epoch 65/1500 - Loss: 0.1865 - Acc: 0.9374 - Val Loss: 0.3754 - Val Acc: 0.8878\n",
      "Epoch 70/1500 - Loss: 0.1932 - Acc: 0.9322 - Val Loss: 0.3737 - Val Acc: 0.8819\n",
      "Epoch 75/1500 - Loss: 0.1580 - Acc: 0.9434 - Val Loss: 0.3715 - Val Acc: 0.8898\n",
      "Epoch 80/1500 - Loss: 0.1470 - Acc: 0.9542 - Val Loss: 0.3911 - Val Acc: 0.8917\n",
      "Epoch 85/1500 - Loss: 0.1602 - Acc: 0.9421 - Val Loss: 0.3743 - Val Acc: 0.8898\n",
      "Epoch 90/1500 - Loss: 0.1436 - Acc: 0.9525 - Val Loss: 0.3372 - Val Acc: 0.9075\n",
      "Epoch 95/1500 - Loss: 0.1336 - Acc: 0.9495 - Val Loss: 0.3473 - Val Acc: 0.8976\n",
      "Epoch 100/1500 - Loss: 0.1265 - Acc: 0.9551 - Val Loss: 0.4239 - Val Acc: 0.8898\n",
      "Epoch 105/1500 - Loss: 0.1289 - Acc: 0.9585 - Val Loss: 0.4167 - Val Acc: 0.8898\n",
      "Epoch 110/1500 - Loss: 0.0978 - Acc: 0.9702 - Val Loss: 0.3684 - Val Acc: 0.9154\n",
      "Epoch 115/1500 - Loss: 0.1044 - Acc: 0.9637 - Val Loss: 0.3918 - Val Acc: 0.8937\n",
      "Epoch 120/1500 - Loss: 0.0937 - Acc: 0.9663 - Val Loss: 0.4074 - Val Acc: 0.9035\n",
      "Epoch 125/1500 - Loss: 0.1072 - Acc: 0.9633 - Val Loss: 0.3935 - Val Acc: 0.9016\n",
      "Epoch 130/1500 - Loss: 0.1156 - Acc: 0.9637 - Val Loss: 0.3616 - Val Acc: 0.8917\n",
      "Epoch 135/1500 - Loss: 0.0953 - Acc: 0.9689 - Val Loss: 0.3938 - Val Acc: 0.8917\n",
      "Epoch 140/1500 - Loss: 0.0869 - Acc: 0.9702 - Val Loss: 0.3996 - Val Acc: 0.8976\n",
      "Epoch 145/1500 - Loss: 0.0720 - Acc: 0.9767 - Val Loss: 0.4013 - Val Acc: 0.9016\n",
      "Epoch 150/1500 - Loss: 0.0757 - Acc: 0.9715 - Val Loss: 0.3842 - Val Acc: 0.9134\n",
      "Epoch 155/1500 - Loss: 0.0781 - Acc: 0.9741 - Val Loss: 0.4041 - Val Acc: 0.8996\n",
      "Epoch 160/1500 - Loss: 0.0789 - Acc: 0.9780 - Val Loss: 0.4558 - Val Acc: 0.8819\n",
      "Epoch 165/1500 - Loss: 0.0700 - Acc: 0.9767 - Val Loss: 0.4202 - Val Acc: 0.9055\n",
      "Epoch 170/1500 - Loss: 0.0706 - Acc: 0.9797 - Val Loss: 0.3721 - Val Acc: 0.9154\n",
      "Epoch 175/1500 - Loss: 0.0884 - Acc: 0.9715 - Val Loss: 0.3982 - Val Acc: 0.9114\n",
      "Epoch 180/1500 - Loss: 0.0639 - Acc: 0.9775 - Val Loss: 0.3965 - Val Acc: 0.9094\n",
      "Epoch 185/1500 - Loss: 0.0765 - Acc: 0.9706 - Val Loss: 0.3750 - Val Acc: 0.9154\n",
      "Epoch 190/1500 - Loss: 0.0469 - Acc: 0.9832 - Val Loss: 0.4011 - Val Acc: 0.9114\n",
      "Epoch 195/1500 - Loss: 0.0644 - Acc: 0.9793 - Val Loss: 0.4055 - Val Acc: 0.9114\n",
      "Epoch 200/1500 - Loss: 0.0654 - Acc: 0.9745 - Val Loss: 0.4204 - Val Acc: 0.9094\n",
      "Epoch 205/1500 - Loss: 0.0517 - Acc: 0.9819 - Val Loss: 0.4265 - Val Acc: 0.9075\n",
      "Epoch 210/1500 - Loss: 0.0710 - Acc: 0.9741 - Val Loss: 0.4381 - Val Acc: 0.8996\n",
      "Epoch 215/1500 - Loss: 0.0521 - Acc: 0.9810 - Val Loss: 0.3909 - Val Acc: 0.9094\n",
      "Epoch 220/1500 - Loss: 0.0579 - Acc: 0.9814 - Val Loss: 0.4104 - Val Acc: 0.9035\n",
      "Epoch 225/1500 - Loss: 0.0500 - Acc: 0.9849 - Val Loss: 0.4329 - Val Acc: 0.9094\n",
      "Epoch 230/1500 - Loss: 0.0507 - Acc: 0.9836 - Val Loss: 0.3780 - Val Acc: 0.9154\n",
      "Epoch 235/1500 - Loss: 0.0548 - Acc: 0.9775 - Val Loss: 0.4539 - Val Acc: 0.9094\n",
      "Epoch 240/1500 - Loss: 0.0447 - Acc: 0.9853 - Val Loss: 0.4362 - Val Acc: 0.9134\n",
      "Epoch 245/1500 - Loss: 0.0556 - Acc: 0.9775 - Val Loss: 0.3958 - Val Acc: 0.9173\n",
      "Epoch 250/1500 - Loss: 0.0425 - Acc: 0.9827 - Val Loss: 0.4529 - Val Acc: 0.8957\n",
      "Epoch 255/1500 - Loss: 0.0393 - Acc: 0.9862 - Val Loss: 0.4561 - Val Acc: 0.9016\n",
      "Epoch 260/1500 - Loss: 0.0417 - Acc: 0.9853 - Val Loss: 0.4525 - Val Acc: 0.9075\n",
      "Epoch 265/1500 - Loss: 0.0468 - Acc: 0.9849 - Val Loss: 0.4281 - Val Acc: 0.9094\n",
      "Epoch 270/1500 - Loss: 0.0627 - Acc: 0.9762 - Val Loss: 0.5029 - Val Acc: 0.9016\n",
      "Epoch 275/1500 - Loss: 0.0465 - Acc: 0.9840 - Val Loss: 0.4824 - Val Acc: 0.9075\n",
      "Epoch 280/1500 - Loss: 0.0572 - Acc: 0.9827 - Val Loss: 0.5183 - Val Acc: 0.9055\n",
      "Epoch 285/1500 - Loss: 0.0442 - Acc: 0.9836 - Val Loss: 0.4729 - Val Acc: 0.9016\n",
      "Epoch 290/1500 - Loss: 0.0511 - Acc: 0.9844 - Val Loss: 0.4774 - Val Acc: 0.9016\n",
      "Epoch 295/1500 - Loss: 0.0426 - Acc: 0.9849 - Val Loss: 0.5110 - Val Acc: 0.9016\n",
      "Epoch 300/1500 - Loss: 0.0449 - Acc: 0.9853 - Val Loss: 0.4347 - Val Acc: 0.9075\n",
      "Epoch 305/1500 - Loss: 0.0480 - Acc: 0.9840 - Val Loss: 0.4519 - Val Acc: 0.9055\n",
      "Epoch 310/1500 - Loss: 0.0416 - Acc: 0.9836 - Val Loss: 0.4654 - Val Acc: 0.9094\n",
      "Epoch 315/1500 - Loss: 0.0490 - Acc: 0.9836 - Val Loss: 0.5131 - Val Acc: 0.9016\n",
      "Epoch 320/1500 - Loss: 0.0443 - Acc: 0.9840 - Val Loss: 0.5172 - Val Acc: 0.9016\n",
      "Epoch 325/1500 - Loss: 0.0491 - Acc: 0.9870 - Val Loss: 0.4724 - Val Acc: 0.9055\n",
      "Epoch 330/1500 - Loss: 0.0495 - Acc: 0.9814 - Val Loss: 0.4397 - Val Acc: 0.9016\n",
      "Epoch 335/1500 - Loss: 0.0417 - Acc: 0.9840 - Val Loss: 0.4774 - Val Acc: 0.9075\n",
      "Epoch 340/1500 - Loss: 0.0340 - Acc: 0.9870 - Val Loss: 0.4992 - Val Acc: 0.9016\n",
      "Epoch 345/1500 - Loss: 0.0246 - Acc: 0.9905 - Val Loss: 0.4868 - Val Acc: 0.9094\n",
      "Epoch 350/1500 - Loss: 0.0449 - Acc: 0.9870 - Val Loss: 0.4394 - Val Acc: 0.9075\n",
      "Epoch 355/1500 - Loss: 0.0333 - Acc: 0.9892 - Val Loss: 0.4537 - Val Acc: 0.9055\n",
      "Epoch 360/1500 - Loss: 0.0480 - Acc: 0.9840 - Val Loss: 0.4252 - Val Acc: 0.9075\n",
      "Epoch 365/1500 - Loss: 0.0370 - Acc: 0.9892 - Val Loss: 0.4178 - Val Acc: 0.9173\n",
      "Epoch 370/1500 - Loss: 0.0492 - Acc: 0.9819 - Val Loss: 0.4422 - Val Acc: 0.9134\n",
      "Epoch 375/1500 - Loss: 0.0437 - Acc: 0.9870 - Val Loss: 0.4273 - Val Acc: 0.9291\n",
      "Epoch 380/1500 - Loss: 0.0351 - Acc: 0.9870 - Val Loss: 0.4456 - Val Acc: 0.9173\n",
      "Epoch 385/1500 - Loss: 0.0413 - Acc: 0.9875 - Val Loss: 0.4569 - Val Acc: 0.9252\n",
      "Epoch 390/1500 - Loss: 0.0273 - Acc: 0.9914 - Val Loss: 0.4563 - Val Acc: 0.9114\n",
      "Epoch 395/1500 - Loss: 0.0296 - Acc: 0.9918 - Val Loss: 0.4523 - Val Acc: 0.9154\n",
      "Epoch 400/1500 - Loss: 0.0380 - Acc: 0.9892 - Val Loss: 0.4383 - Val Acc: 0.9173\n",
      "Epoch 405/1500 - Loss: 0.0264 - Acc: 0.9918 - Val Loss: 0.4476 - Val Acc: 0.9114\n",
      "Epoch 410/1500 - Loss: 0.0486 - Acc: 0.9836 - Val Loss: 0.5113 - Val Acc: 0.9055\n",
      "Epoch 415/1500 - Loss: 0.0348 - Acc: 0.9857 - Val Loss: 0.4449 - Val Acc: 0.9134\n",
      "Epoch 420/1500 - Loss: 0.0268 - Acc: 0.9892 - Val Loss: 0.5145 - Val Acc: 0.9075\n",
      "Epoch 425/1500 - Loss: 0.0198 - Acc: 0.9935 - Val Loss: 0.4915 - Val Acc: 0.9075\n",
      "Epoch 430/1500 - Loss: 0.0311 - Acc: 0.9905 - Val Loss: 0.4737 - Val Acc: 0.9193\n",
      "Epoch 435/1500 - Loss: 0.0241 - Acc: 0.9927 - Val Loss: 0.4891 - Val Acc: 0.9232\n",
      "Epoch 440/1500 - Loss: 0.0194 - Acc: 0.9927 - Val Loss: 0.4185 - Val Acc: 0.9173\n",
      "Epoch 445/1500 - Loss: 0.0269 - Acc: 0.9922 - Val Loss: 0.5046 - Val Acc: 0.9114\n",
      "Epoch 450/1500 - Loss: 0.0400 - Acc: 0.9836 - Val Loss: 0.4869 - Val Acc: 0.9114\n",
      "Epoch 455/1500 - Loss: 0.0327 - Acc: 0.9918 - Val Loss: 0.4603 - Val Acc: 0.9213\n",
      "Epoch 460/1500 - Loss: 0.0326 - Acc: 0.9870 - Val Loss: 0.4402 - Val Acc: 0.9114\n",
      "Epoch 465/1500 - Loss: 0.0317 - Acc: 0.9896 - Val Loss: 0.4780 - Val Acc: 0.9134\n",
      "Epoch 470/1500 - Loss: 0.0448 - Acc: 0.9870 - Val Loss: 0.4703 - Val Acc: 0.9213\n",
      "Epoch 475/1500 - Loss: 0.0372 - Acc: 0.9901 - Val Loss: 0.4399 - Val Acc: 0.9193\n",
      "Epoch 480/1500 - Loss: 0.0305 - Acc: 0.9875 - Val Loss: 0.4239 - Val Acc: 0.9173\n",
      "Epoch 485/1500 - Loss: 0.0328 - Acc: 0.9888 - Val Loss: 0.4299 - Val Acc: 0.9114\n",
      "Epoch 490/1500 - Loss: 0.0297 - Acc: 0.9896 - Val Loss: 0.4162 - Val Acc: 0.9094\n",
      "Epoch 495/1500 - Loss: 0.0243 - Acc: 0.9918 - Val Loss: 0.4487 - Val Acc: 0.9114\n",
      "Epoch 500/1500 - Loss: 0.0339 - Acc: 0.9892 - Val Loss: 0.4802 - Val Acc: 0.9154\n",
      "Epoch 505/1500 - Loss: 0.0342 - Acc: 0.9914 - Val Loss: 0.4089 - Val Acc: 0.9114\n",
      "Epoch 510/1500 - Loss: 0.0325 - Acc: 0.9896 - Val Loss: 0.4359 - Val Acc: 0.9173\n",
      "Epoch 515/1500 - Loss: 0.0210 - Acc: 0.9935 - Val Loss: 0.5097 - Val Acc: 0.9193\n",
      "Epoch 520/1500 - Loss: 0.0263 - Acc: 0.9918 - Val Loss: 0.4971 - Val Acc: 0.9035\n",
      "Epoch 525/1500 - Loss: 0.0390 - Acc: 0.9896 - Val Loss: 0.4240 - Val Acc: 0.9311\n",
      "Epoch 530/1500 - Loss: 0.0282 - Acc: 0.9909 - Val Loss: 0.4472 - Val Acc: 0.9252\n",
      "Epoch 535/1500 - Loss: 0.0217 - Acc: 0.9931 - Val Loss: 0.4397 - Val Acc: 0.9213\n",
      "Epoch 540/1500 - Loss: 0.0282 - Acc: 0.9892 - Val Loss: 0.5061 - Val Acc: 0.9134\n",
      "Epoch 545/1500 - Loss: 0.0232 - Acc: 0.9909 - Val Loss: 0.5801 - Val Acc: 0.9075\n",
      "Epoch 550/1500 - Loss: 0.0204 - Acc: 0.9922 - Val Loss: 0.5727 - Val Acc: 0.9035\n",
      "Epoch 555/1500 - Loss: 0.0276 - Acc: 0.9901 - Val Loss: 0.4945 - Val Acc: 0.9134\n",
      "Epoch 560/1500 - Loss: 0.0375 - Acc: 0.9883 - Val Loss: 0.5990 - Val Acc: 0.9173\n",
      "Epoch 565/1500 - Loss: 0.0291 - Acc: 0.9883 - Val Loss: 0.5678 - Val Acc: 0.9213\n",
      "Epoch 570/1500 - Loss: 0.0264 - Acc: 0.9905 - Val Loss: 0.4979 - Val Acc: 0.9154\n",
      "Epoch 575/1500 - Loss: 0.0238 - Acc: 0.9914 - Val Loss: 0.4601 - Val Acc: 0.9213\n",
      "Epoch 580/1500 - Loss: 0.0318 - Acc: 0.9892 - Val Loss: 0.4609 - Val Acc: 0.9154\n",
      "Epoch 585/1500 - Loss: 0.0249 - Acc: 0.9940 - Val Loss: 0.4475 - Val Acc: 0.9114\n",
      "Epoch 590/1500 - Loss: 0.0269 - Acc: 0.9875 - Val Loss: 0.4769 - Val Acc: 0.9055\n",
      "Epoch 595/1500 - Loss: 0.0215 - Acc: 0.9918 - Val Loss: 0.4934 - Val Acc: 0.9094\n",
      "Epoch 600/1500 - Loss: 0.0255 - Acc: 0.9918 - Val Loss: 0.4946 - Val Acc: 0.9075\n",
      "Epoch 605/1500 - Loss: 0.0230 - Acc: 0.9905 - Val Loss: 0.4671 - Val Acc: 0.9094\n",
      "Epoch 610/1500 - Loss: 0.0328 - Acc: 0.9883 - Val Loss: 0.4800 - Val Acc: 0.9114\n",
      "Epoch 615/1500 - Loss: 0.0255 - Acc: 0.9927 - Val Loss: 0.4754 - Val Acc: 0.9075\n",
      "Epoch 620/1500 - Loss: 0.0177 - Acc: 0.9952 - Val Loss: 0.5163 - Val Acc: 0.9094\n",
      "Epoch 625/1500 - Loss: 0.0204 - Acc: 0.9922 - Val Loss: 0.4840 - Val Acc: 0.9134\n",
      "Epoch 630/1500 - Loss: 0.0189 - Acc: 0.9948 - Val Loss: 0.5098 - Val Acc: 0.9114\n",
      "Epoch 635/1500 - Loss: 0.0220 - Acc: 0.9931 - Val Loss: 0.5021 - Val Acc: 0.9173\n",
      "Epoch 640/1500 - Loss: 0.0294 - Acc: 0.9905 - Val Loss: 0.4976 - Val Acc: 0.9094\n",
      "Epoch 645/1500 - Loss: 0.0221 - Acc: 0.9944 - Val Loss: 0.4558 - Val Acc: 0.9134\n",
      "Epoch 650/1500 - Loss: 0.0211 - Acc: 0.9918 - Val Loss: 0.4767 - Val Acc: 0.9173\n",
      "Epoch 655/1500 - Loss: 0.0210 - Acc: 0.9922 - Val Loss: 0.5060 - Val Acc: 0.9134\n",
      "Epoch 660/1500 - Loss: 0.0152 - Acc: 0.9948 - Val Loss: 0.5259 - Val Acc: 0.9232\n",
      "Epoch 665/1500 - Loss: 0.0175 - Acc: 0.9940 - Val Loss: 0.4847 - Val Acc: 0.9154\n",
      "Epoch 670/1500 - Loss: 0.0206 - Acc: 0.9918 - Val Loss: 0.4827 - Val Acc: 0.9173\n",
      "Epoch 675/1500 - Loss: 0.0237 - Acc: 0.9922 - Val Loss: 0.4787 - Val Acc: 0.9134\n",
      "Epoch 680/1500 - Loss: 0.0312 - Acc: 0.9931 - Val Loss: 0.4596 - Val Acc: 0.9252\n",
      "Epoch 685/1500 - Loss: 0.0267 - Acc: 0.9914 - Val Loss: 0.4633 - Val Acc: 0.9154\n",
      "Epoch 690/1500 - Loss: 0.0243 - Acc: 0.9931 - Val Loss: 0.4736 - Val Acc: 0.9272\n",
      "Epoch 695/1500 - Loss: 0.0142 - Acc: 0.9965 - Val Loss: 0.4457 - Val Acc: 0.9193\n",
      "Epoch 700/1500 - Loss: 0.0278 - Acc: 0.9940 - Val Loss: 0.4687 - Val Acc: 0.9213\n",
      "Epoch 705/1500 - Loss: 0.0240 - Acc: 0.9944 - Val Loss: 0.4855 - Val Acc: 0.9173\n",
      "Epoch 710/1500 - Loss: 0.0259 - Acc: 0.9927 - Val Loss: 0.5065 - Val Acc: 0.9075\n",
      "Epoch 715/1500 - Loss: 0.0231 - Acc: 0.9940 - Val Loss: 0.4989 - Val Acc: 0.9114\n",
      "Epoch 720/1500 - Loss: 0.0191 - Acc: 0.9935 - Val Loss: 0.5746 - Val Acc: 0.9154\n",
      "Epoch 725/1500 - Loss: 0.0322 - Acc: 0.9918 - Val Loss: 0.5322 - Val Acc: 0.9134\n",
      "Epoch 730/1500 - Loss: 0.0285 - Acc: 0.9927 - Val Loss: 0.5512 - Val Acc: 0.9193\n",
      "Epoch 735/1500 - Loss: 0.0211 - Acc: 0.9931 - Val Loss: 0.5471 - Val Acc: 0.9114\n",
      "Epoch 740/1500 - Loss: 0.0188 - Acc: 0.9944 - Val Loss: 0.5595 - Val Acc: 0.9134\n",
      "Epoch 745/1500 - Loss: 0.0179 - Acc: 0.9944 - Val Loss: 0.5566 - Val Acc: 0.9193\n",
      "Epoch 750/1500 - Loss: 0.0209 - Acc: 0.9935 - Val Loss: 0.5414 - Val Acc: 0.9134\n",
      "Epoch 755/1500 - Loss: 0.0167 - Acc: 0.9944 - Val Loss: 0.5620 - Val Acc: 0.9055\n",
      "Epoch 760/1500 - Loss: 0.0213 - Acc: 0.9935 - Val Loss: 0.5296 - Val Acc: 0.9094\n",
      "Epoch 765/1500 - Loss: 0.0228 - Acc: 0.9914 - Val Loss: 0.4970 - Val Acc: 0.9213\n",
      "Epoch 770/1500 - Loss: 0.0218 - Acc: 0.9909 - Val Loss: 0.4709 - Val Acc: 0.9291\n",
      "Epoch 775/1500 - Loss: 0.0274 - Acc: 0.9918 - Val Loss: 0.5532 - Val Acc: 0.9075\n",
      "Epoch 780/1500 - Loss: 0.0194 - Acc: 0.9927 - Val Loss: 0.5458 - Val Acc: 0.9193\n",
      "Epoch 785/1500 - Loss: 0.0139 - Acc: 0.9940 - Val Loss: 0.6429 - Val Acc: 0.9173\n",
      "Epoch 790/1500 - Loss: 0.0116 - Acc: 0.9944 - Val Loss: 0.4969 - Val Acc: 0.9154\n",
      "Epoch 795/1500 - Loss: 0.0173 - Acc: 0.9965 - Val Loss: 0.6168 - Val Acc: 0.9114\n",
      "Epoch 800/1500 - Loss: 0.0181 - Acc: 0.9952 - Val Loss: 0.6289 - Val Acc: 0.9055\n",
      "Epoch 805/1500 - Loss: 0.0199 - Acc: 0.9957 - Val Loss: 0.5744 - Val Acc: 0.9193\n",
      "Epoch 810/1500 - Loss: 0.0262 - Acc: 0.9927 - Val Loss: 0.5820 - Val Acc: 0.9134\n",
      "Epoch 815/1500 - Loss: 0.0211 - Acc: 0.9940 - Val Loss: 0.5638 - Val Acc: 0.9193\n",
      "Epoch 820/1500 - Loss: 0.0222 - Acc: 0.9927 - Val Loss: 0.4902 - Val Acc: 0.9232\n",
      "Epoch 825/1500 - Loss: 0.0284 - Acc: 0.9918 - Val Loss: 0.5062 - Val Acc: 0.9173\n",
      "Epoch 830/1500 - Loss: 0.0143 - Acc: 0.9957 - Val Loss: 0.5496 - Val Acc: 0.9094\n",
      "Epoch 835/1500 - Loss: 0.0275 - Acc: 0.9935 - Val Loss: 0.5499 - Val Acc: 0.9193\n",
      "Epoch 840/1500 - Loss: 0.0176 - Acc: 0.9957 - Val Loss: 0.5543 - Val Acc: 0.9114\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_path = \"/kaggle/input/dataset/FaceTrain(arranged).xlsx\"\n",
    "    test_path = \"/kaggle/input/dataset/FaceVal(arranged).xlsx\"\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, label_encoder = load_emotion_data(train_path, test_path)\n",
    "\n",
    "    emotion_detector = EmotionDetectionPyTorch(X_train, X_test, y_train, y_test, label_encoder)\n",
    "    \n",
    "    model = emotion_detector.run_experiment() \n",
    "    \n",
    "    emotion_detector.save_model(model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7125212,
     "sourceId": 11379962,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7164051,
     "sourceId": 11437137,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7166384,
     "sourceId": 11440146,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
