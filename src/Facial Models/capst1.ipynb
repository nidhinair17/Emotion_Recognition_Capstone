{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_ravdess_data():\n",
    "    # emotion_files = {        \n",
    "    #     'neutral': \"C:/Nini/Capstone/CSV_Files/Facial data/Neutral.xlsx\",\n",
    "    #     'calm': \"C:/Nini/Capstone/CSV_Files/Facial data/Calm.xlsx\",\n",
    "    #     'happy': \"C:/Nini/Capstone/CSV_Files/Facial data/Happy.xlsx\",\n",
    "    #     'sad': \"C:/Nini/Capstone/CSV_Files/Facial data/Sad.xlsx\",\n",
    "    #     'angry': \"C:/Nini/Capstone/CSV_Files/Facial data/Angry.xlsx\",\n",
    "    #     'fearful': \"C:/Nini/Capstone/CSV_Files/Facial data/Fearful.xlsx\",\n",
    "    #     'disgust': \"C:/Nini/Capstone/CSV_Files/Facial data/Disgusted.xlsx\",\n",
    "    #     'surprised': \"C:/Nini/Capstone/CSV_Files/Facial data/Surprised.xlsx\"\n",
    "    # }\n",
    "\n",
    "    # all_data = []\n",
    "    \n",
    "    # for emotion, file_path in emotion_files.items():\n",
    "    #     try:\n",
    "    #         df = pd.read_excel(file_path)\n",
    "    #         df['emotion'] = emotion\n",
    "    #         all_data.append(df)\n",
    "    #         print(f\"Loaded {emotion} data: {len(df)} samples\")\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error loading {emotion} file: {str(e)}\")\n",
    "    \n",
    "    # combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # feature_columns = ['face1_face_x', 'face_y', 'face_width', 'face_height', 'face_aspect_ratio',\n",
    "    #                    'left_eye_x', 'left_eye_y', 'left_eye_width', 'left_eye_height',\n",
    "    #                    'right_eye_x', 'right_eye_y', 'right_eye_width', 'right_eye_height',\n",
    "    #                    'eye_separation', 'mouth_x', 'mouth_y', 'mouth_width', 'mouth_height',\n",
    "    #                    'mouth_aspect_ratio', 'avg_intensity', 'intensity_variance']\n",
    "    \n",
    "    # print(\"\\nDataset Statistics:\")\n",
    "    # print(f\"Total samples: {len(combined_df)}\")\n",
    "    # print(\"\\nSamples per emotion:\")\n",
    "    # print(combined_df['emotion'].value_counts())\n",
    "    \n",
    "    # X = combined_df[feature_columns].values\n",
    "    \n",
    "    # label_encoder = LabelEncoder()\n",
    "    # y = label_encoder.fit_transform(combined_df['emotion'])\n",
    "    \n",
    "    # return X, y, label_encoder\n",
    "    train_data = pd.read_excel(\"C:/Nini/Capstone/CSV_Files/Facial data/Train.xlsx\")\n",
    "    feature_columns = ['face_x', 'face_y', 'face_width', 'face_height', 'face_aspect_ratio',\n",
    "                        'left_eye_x', 'left_eye_y', 'left_eye_width', 'left_eye_height',\n",
    "                        'right_eye_x', 'right_eye_y', 'right_eye_width', 'right_eye_height',\n",
    "                        'eye_separation', 'mouth_x', 'mouth_y', 'mouth_width', 'mouth_height',\n",
    "                        'mouth_aspect_ratio', 'avg_intensity', 'intensity_variance']\n",
    "    X_train = train_data[feature_columns].values\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train = label_encoder.fit_transform(train_data['emotion'])\n",
    "\n",
    "    test_data = pd.read_excel(\"C:/Nini/Capstone/CSV_Files/Facial data/Test.xlsx\")\n",
    "    print(train_data.isnull().sum())\n",
    "    print(train_data[train_data.isnull().any(axis=1)])\n",
    "    # print(test_data[feature_columns].dtypes)\n",
    "    print(test_data.head())\n",
    "    X_test = test_data[feature_columns].values\n",
    "    y_test = label_encoder.fit_transform(test_data['emotion'])\n",
    "\n",
    "    print(X_train.dtype)\n",
    "    print(X_test.dtype)\n",
    "    print(y_train.dtype)\n",
    "    print(y_test.dtype)\n",
    "    return X_train, X_test, y_train, y_test, label_encoder\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = pd.read_excel(\"C:/Nini/Capstone/CSV_Files/Facial data/Train.xlsx\")\n",
    "# feature_columns = ['face_x', 'face_y', 'face_width', 'face_height', 'face_aspect_ratio',\n",
    "#                        'left_eye_x', 'left_eye_y', 'left_eye_width', 'left_eye_height',\n",
    "#                        'right_eye_x', 'right_eye_y', 'right_eye_width', 'right_eye_height',\n",
    "#                        'eye_separation', 'mouth_x', 'mouth_y', 'mouth_width', 'mouth_height',\n",
    "#                        'mouth_aspect_ratio', 'avg_intensity', 'intensity_variance']\n",
    "# X_train = train_data[feature_columns].values\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(train_data['emotion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data = pd.read_excel(\"C:/Nini/Capstone/CSV_Files/Facial data/Test.xlsx\")\n",
    "# X_test = test_data[feature_columns].values\n",
    "# y_test = label_encoder.fit_transform(train_data['emotion'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=192, num_classes=8, dropout_rate=0.3):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "        # Simplified architecture - one bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Simple dense layers\n",
    "        self.bn = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Get last time step or squeeze\n",
    "        if lstm_out.size(1) == 1:\n",
    "            lstm_out = lstm_out.squeeze(1)\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# # class EmotionLSTM(nn.Module):\n",
    "# # #     def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=8, dropout=0.5):\n",
    "# # #         super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "# # #         self.lstm = nn.LSTM(\n",
    "# # #             input_size=input_size,\n",
    "# # #             hidden_size=hidden_size,\n",
    "# # #             num_layers=num_layers,\n",
    "# # #             batch_first=True,\n",
    "# # #             dropout=dropout\n",
    "# # #         )\n",
    "        \n",
    "# # #         self.fc1 = nn.Linear(hidden_size, 64)\n",
    "# # #         self.dropout = nn.Dropout(dropout)\n",
    "# # #         self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "# # #     def forward(self, x):\n",
    "# # #         # x shape: (batch_size, seq_length, input_size)\n",
    "# # #         out, _ = self.lstm(x)  # output from all time steps\n",
    "# # #         out = out[:, -1, :]    # Take output of the last time step\n",
    "\n",
    "# # #         out = F.relu(self.fc1(out))\n",
    "# # #         out = self.dropout(out)\n",
    "# # #         out = self.fc2(out)\n",
    "# # #         return out\n",
    "\n",
    "# # # class EmotionLSTM(nn.Module):\n",
    "# # #     def __init__(self, input_size, hidden_size=128, num_layers=2, num_classes=6, dropout=0.3):\n",
    "# # #         super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "# # #         self.lstm = nn.LSTM(\n",
    "# # #             input_size=input_size,\n",
    "# # #             hidden_size=hidden_size,\n",
    "# # #             num_layers=num_layers,\n",
    "# # #             batch_first=True,\n",
    "# # #             dropout=dropout\n",
    "# # #         )\n",
    "        \n",
    "# # #         self.fc1 = nn.Linear(hidden_size, 64)\n",
    "# # #         self.dropout = nn.Dropout(dropout)\n",
    "# # #         self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "# # #     def forward(self, x):\n",
    "# # #         # x shape: (batch_size, seq_length, input_size)\n",
    "# # #         out, _ = self.lstm(x)  # output from all time steps\n",
    "# # #         out = out[:, -1, :]    # Take output of the last time step\n",
    "\n",
    "# # #         out = F.relu(self.fc1(out))\n",
    "# # #         out = self.dropout(out)\n",
    "# # #         out = self.fc2(out)\n",
    "# # #         return out\n",
    "\n",
    "# # class EmotionLSTM(nn.Module):\n",
    "# #     def __init__(self, input_size, hidden_size=256, num_layers=2, num_classes=8, dropout=0.5, bidirectional=True):\n",
    "# #         super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "# #         # Make LSTM bidirectional for better temporal feature understanding\n",
    "# #         self.lstm = nn.LSTM(\n",
    "# #             input_size=input_size,\n",
    "# #             hidden_size=hidden_size,\n",
    "# #             num_layers=num_layers,\n",
    "# #             batch_first=True,\n",
    "# #             dropout=dropout if num_layers > 1 else 0,\n",
    "# #             bidirectional=bidirectional\n",
    "# #         )\n",
    "        \n",
    "# #         # Adjust for bidirectional output\n",
    "# #         lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n",
    "        \n",
    "# #         # Add batch normalization for better training stability\n",
    "# #         self.batch_norm = nn.BatchNorm1d(lstm_output_size)\n",
    "        \n",
    "# #         # Improved fully connected layers\n",
    "# #         self.fc1 = nn.Linear(lstm_output_size, 128)\n",
    "# #         self.dropout1 = nn.Dropout(dropout)\n",
    "# #         self.fc2 = nn.Linear(128, 64)\n",
    "# #         self.dropout2 = nn.Dropout(dropout)\n",
    "# #         self.fc3 = nn.Linear(64, num_classes)\n",
    "        \n",
    "# #     def forward(self, x):\n",
    "# #         # x shape: (batch_size, seq_length, input_size)\n",
    "# #         out, _ = self.lstm(x)\n",
    "        \n",
    "# #         # Get output from last time step\n",
    "# #         if self.lstm.bidirectional:\n",
    "# #             # Combine forward and backward last outputs\n",
    "# #             out = out[:, -1, :]\n",
    "# #         else:\n",
    "# #             out = out[:, -1, :]\n",
    "            \n",
    "# #         # Apply batch normalization\n",
    "# #         out = self.batch_norm(out)\n",
    "        \n",
    "# #         # First dense layer with activation and dropout\n",
    "# #         out = F.relu(self.fc1(out))\n",
    "# #         out = self.dropout1(out)\n",
    "        \n",
    "# #         # Second dense layer with activation and dropout\n",
    "# #         out = F.relu(self.fc2(out))\n",
    "# #         out = self.dropout2(out)\n",
    "        \n",
    "# #         # Output layer\n",
    "# #         out = self.fc3(out)\n",
    "        \n",
    "# #         return out\n",
    "\n",
    "# class EmotionLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes=8):\n",
    "#         super(EmotionLSTM, self).__init__()\n",
    "\n",
    "#         self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=128, batch_first=True)\n",
    "#         self.bn1 = nn.BatchNorm1d(1)\n",
    "#         self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "#         self.lstm2 = nn.LSTM(input_size=128, hidden_size=96, batch_first=True)\n",
    "#         self.bn2 = nn.BatchNorm1d(1)\n",
    "#         self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "#         self.lstm3 = nn.LSTM(input_size=96, hidden_size=64, batch_first=True)\n",
    "#         self.bn3 = nn.BatchNorm1d(64)\n",
    "#         self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "#         self.fc1 = nn.Linear(64, 128)\n",
    "#         self.bn4 = nn.BatchNorm1d(128)\n",
    "#         self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.bn5 = nn.BatchNorm1d(64)\n",
    "#         self.dropout5 = nn.Dropout(0.2)\n",
    "\n",
    "#         self.output = nn.Linear(64, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, 1, input_size)\n",
    "#         x, _ = self.lstm1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x, _ = self.lstm2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.dropout2(x)\n",
    "\n",
    "#         x, _ = self.lstm3(x)\n",
    "#         x = x[:, -1, :]  # get last time step\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.dropout3(x)\n",
    "\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.bn4(x)\n",
    "#         x = self.dropout4(x)\n",
    "\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.bn5(x)\n",
    "#         x = self.dropout5(x)\n",
    "\n",
    "#         x = self.output(x)\n",
    "#         return F.softmax(x, dim=1)\n",
    "\n",
    "# class EmotionLSTM(nn.Module):\n",
    "#     def __init__(self, input_size, num_classes=8):\n",
    "#         super(EmotionLSTM, self).__init__()\n",
    "\n",
    "#         self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=128, batch_first=True)\n",
    "#         self.bn1 = nn.BatchNorm1d(1)\n",
    "#         self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "#         self.lstm2 = nn.LSTM(input_size=128, hidden_size=96, batch_first=True)\n",
    "#         self.bn2 = nn.BatchNorm1d(1)\n",
    "#         self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "#         self.lstm3 = nn.LSTM(input_size=96, hidden_size=64, batch_first=True)\n",
    "#         self.bn3 = nn.BatchNorm1d(64)\n",
    "#         self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "#         self.fc1 = nn.Linear(64, 128)\n",
    "#         self.bn4 = nn.BatchNorm1d(128)\n",
    "#         self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "#         self.fc2 = nn.Linear(128, 64)\n",
    "#         self.bn5 = nn.BatchNorm1d(64)\n",
    "#         self.dropout5 = nn.Dropout(0.2)\n",
    "\n",
    "#         self.output = nn.Linear(64, num_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: (batch, 1, input_size)\n",
    "#         x, _ = self.lstm1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = self.dropout1(x)\n",
    "\n",
    "#         x, _ = self.lstm2(x)\n",
    "#         x = self.bn2(x)\n",
    "#         x = self.dropout2(x)\n",
    "\n",
    "#         x, _ = self.lstm3(x)\n",
    "#         x = x[:, -1, :]  # get last time step\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.dropout3(x)\n",
    "\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.bn4(x)\n",
    "#         x = self.dropout4(x)\n",
    "\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         x = self.bn5(x)\n",
    "#         x = self.dropout5(x)\n",
    "\n",
    "#         x = self.output(x)\n",
    "#         return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionDetectionPyTorch:\n",
    "    def __init__(self, X_train,X_test, y_train, y_test, label_encoder):\n",
    "        # Split the data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = X_train, X_test, y_train, y_test\n",
    "        \n",
    "        # Standardize features\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train_scaled = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test_scaled = self.scaler.transform(self.X_test)\n",
    "        \n",
    "        # Reshape for LSTM (samples, sequence_length, features)\n",
    "        self.X_train_reshaped = self.X_train_scaled.reshape(\n",
    "            (self.X_train_scaled.shape[0], 1, self.X_train_scaled.shape[1]))\n",
    "        self.X_test_reshaped = self.X_test_scaled.reshape(\n",
    "            (self.X_test_scaled.shape[0], 1, self.X_test_scaled.shape[1]))\n",
    "        \n",
    "        self.label_encoder = label_encoder\n",
    "        self.num_classes = len(label_encoder.classes_)\n",
    "        \n",
    "        # Set device\n",
    "        # self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        # Create datasets and dataloaders\n",
    "        self.train_dataset = EmotionDataset(self.X_train_reshaped, self.y_train)\n",
    "        self.test_dataset = EmotionDataset(self.X_test_reshaped, self.y_test)\n",
    "        \n",
    "        self.results = {}\n",
    "        \n",
    "        print(f\"\\nTraining set shape: {self.X_train.shape}\")\n",
    "        print(f\"Testing set shape: {self.X_test.shape}\")\n",
    "\n",
    "    def create_dataloaders(self, batch_size=32):\n",
    "        \"\"\"Create DataLoaders for train and test sets\"\"\"\n",
    "        train_loader = DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, test_loader\n",
    "    \n",
    "    def create_lstm_model(self):\n",
    "        \"\"\"Create LSTM model\"\"\"\n",
    "        input_dim = self.X_train.shape[1]  # Number of features\n",
    "        hidden_dim = 128\n",
    "        model = EmotionLSTM(input_size=self.X_train.shape[1])\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, model, train_loader, test_loader, epochs=1000, learning_rate=0.001):\n",
    "        \"\"\"Train the PyTorch model\"\"\"\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # For tracking metrics\n",
    "        train_losses = []\n",
    "        train_accs = []\n",
    "        val_losses = []\n",
    "        val_accs = []\n",
    "        \n",
    "        print(f\"\\nTraining LSTM model...\")\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Backward pass and optimize\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            epoch_acc = correct / total\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accs.append(epoch_acc)\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_running_loss = 0.0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    val_running_loss += loss.item() * inputs.size(0)\n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_epoch_loss = val_running_loss / len(test_loader.dataset)\n",
    "            val_epoch_acc = val_correct / val_total\n",
    "            val_losses.append(val_epoch_loss)\n",
    "            val_accs.append(val_epoch_acc)\n",
    "            \n",
    "            # Print statistics\n",
    "            if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "                      f\"Loss: {epoch_loss:.4f} - Acc: {epoch_acc:.4f} - \"\n",
    "                      f\"Val Loss: {val_epoch_loss:.4f} - Val Acc: {val_epoch_acc:.4f}\")\n",
    "        \n",
    "        history = {\n",
    "            'loss': train_losses,\n",
    "            'accuracy': train_accs,\n",
    "            'val_loss': val_losses,\n",
    "            'val_accuracy': val_accs\n",
    "        }\n",
    "        \n",
    "        return model, history\n",
    "    \n",
    "    def evaluate_model(self, model, test_loader):\n",
    "        \"\"\"Evaluate the model on test data\"\"\"\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Convert indices to emotion labels\n",
    "        pred_emotions = self.label_encoder.inverse_transform(all_preds)\n",
    "        true_emotions = self.label_encoder.inverse_transform(all_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        conf_matrix = confusion_matrix(true_emotions, pred_emotions)\n",
    "        class_report = classification_report(true_emotions, pred_emotions)\n",
    "        \n",
    "        return conf_matrix, class_report, pred_emotions, true_emotions\n",
    "    \n",
    "    def plot_confusion_matrix(self, conf_matrix, title, emotion_labels):\n",
    "        \"\"\"Plot confusion matrix\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=emotion_labels, yticklabels=emotion_labels)\n",
    "        plt.title(title)\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_training_history(self, history, title):\n",
    "        \"\"\"Plot training history\"\"\"\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title(f'{title} - Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['loss'], label='Training Loss')\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{title} - Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def run_experiment(self, model_name=\"LSTM\", batch_size=32, epochs=1000, learning_rate=0.001):\n",
    "        \"\"\"Run a complete experiment\"\"\"\n",
    "        # Create dataloaders\n",
    "        train_loader, test_loader = self.create_dataloaders(batch_size)\n",
    "        \n",
    "        # Create model\n",
    "        model = self.create_lstm_model()\n",
    "        \n",
    "        # Train model\n",
    "        trained_model, history = self.train_model(\n",
    "            model, train_loader, test_loader, epochs, learning_rate\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        conf_matrix, class_report, pred_emotions, true_emotions = self.evaluate_model(\n",
    "            trained_model, test_loader\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        self.results[model_name] = {\n",
    "            'model': trained_model,\n",
    "            'history': history,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'classification_report': class_report,\n",
    "            'predictions': pred_emotions,\n",
    "            'true_labels': true_emotions\n",
    "        }\n",
    "        \n",
    "        # Plot results\n",
    "        self.plot_training_history(history, model_name)\n",
    "        self.plot_confusion_matrix(\n",
    "            conf_matrix, f'{model_name} Confusion Matrix', \n",
    "            self.label_encoder.classes_\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{model_name} Classification Report:\")\n",
    "        print(class_report)\n",
    "        \n",
    "        return trained_model\n",
    "    \n",
    "    def print_comparison(self):\n",
    "        \"\"\"Print comparison of different models\"\"\"\n",
    "        print(\"\\nModel Comparison Results\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        summary_data = []\n",
    "        \n",
    "        for model_name, result in self.results.items():\n",
    "            print(f\"\\n{model_name} Results\")\n",
    "            print(\"-\" * 30)\n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(result['classification_report'])\n",
    "            \n",
    "            val_acc = result['history']['val_accuracy'][-1]\n",
    "            val_loss = result['history']['val_loss'][-1]\n",
    "            \n",
    "            summary_data.append({\n",
    "                'Model': model_name,\n",
    "                'Validation Accuracy': val_acc,\n",
    "                'Validation Loss': val_loss\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(\"\\nSummary of Results:\")\n",
    "        print(summary_df.to_string(index=False))\n",
    "\n",
    "    def save_model(self, model, path='/Users/smriti/Desktop/emotion_lstm_model.pth'):\n",
    "        \"\"\"Save the PyTorch model\"\"\"\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path='/Users/smriti/Desktop/emotion_lstm_model.pth'):\n",
    "        \"\"\"Load a saved PyTorch model\"\"\"\n",
    "        model = self.create_lstm_model()\n",
    "        model.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_name            0\n",
      "frame                 0\n",
      "face_x                0\n",
      "face_y                0\n",
      "face_width            0\n",
      "face_height           0\n",
      "face_aspect_ratio     0\n",
      "left_eye_x            0\n",
      "left_eye_y            0\n",
      "left_eye_width        0\n",
      "left_eye_height       0\n",
      "right_eye_x           0\n",
      "right_eye_y           0\n",
      "right_eye_width       0\n",
      "right_eye_height      0\n",
      "eye_separation        0\n",
      "mouth_x               0\n",
      "mouth_y               0\n",
      "mouth_width           0\n",
      "mouth_height          0\n",
      "mouth_aspect_ratio    0\n",
      "avg_intensity         0\n",
      "intensity_variance    0\n",
      "emotion               0\n",
      "dtype: int64\n",
      "Empty DataFrame\n",
      "Columns: [video_name, frame, face_x, face_y, face_width, face_height, face_aspect_ratio, left_eye_x, left_eye_y, left_eye_width, left_eye_height, right_eye_x, right_eye_y, right_eye_width, right_eye_height, eye_separation, mouth_x, mouth_y, mouth_width, mouth_height, mouth_aspect_ratio, avg_intensity, intensity_variance, emotion]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 24 columns]\n",
      "                 video_name  frame    face_x    face_y  face_width  \\\n",
      "0  01-01-05-01-02-02-13.mp4      0  0.338281  0.231944    0.301563   \n",
      "1  01-01-05-01-02-02-13.mp4      1  0.340625  0.236111    0.295312   \n",
      "2  01-01-05-01-02-02-13.mp4      2  0.340625  0.233333    0.296875   \n",
      "3  01-01-05-01-02-02-13.mp4      3  0.339844  0.231944    0.300000   \n",
      "4  01-01-05-01-02-02-13.mp4      4  0.339844  0.233333    0.298438   \n",
      "\n",
      "   face_height  face_aspect_ratio  left_eye_x  left_eye_y  left_eye_width  \\\n",
      "0     0.536111                  1    0.176166    0.297927        0.220207   \n",
      "1     0.525000                  1    0.198413    0.304233        0.203704   \n",
      "2     0.527778                  1    0.194737    0.307895        0.205263   \n",
      "3     0.533333                  1    0.192708    0.302083        0.213542   \n",
      "4     0.530556                  1    0.193717    0.303665        0.209424   \n",
      "\n",
      "   ...  right_eye_height  eye_separation   mouth_x   mouth_y  mouth_width  \\\n",
      "0  ...          0.191710        0.358808  0.303109  0.738342     0.458549   \n",
      "1  ...          0.071429        0.129630  0.296296  0.738095     0.468254   \n",
      "2  ...          0.210526        0.357895  0.294737  0.736842     0.465789   \n",
      "3  ...          0.062500        0.130208  0.299479  0.734375     0.460938   \n",
      "4  ...          0.206806        0.357330  0.308901  0.738220     0.463351   \n",
      "\n",
      "   mouth_height  mouth_aspect_ratio  avg_intensity  intensity_variance  \\\n",
      "0      0.227979            2.011364     118.982178         2767.884692   \n",
      "1      0.232804            2.011364     116.905273         2501.524230   \n",
      "2      0.231579            2.011364     117.187500         2524.956055   \n",
      "3      0.229167            2.011364     118.975830         2656.301906   \n",
      "4      0.230366            2.011364     118.092041         2607.638257   \n",
      "\n",
      "   emotion  \n",
      "0    angry  \n",
      "1    angry  \n",
      "2    angry  \n",
      "3    angry  \n",
      "4    angry  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "float64\n",
      "float64\n",
      "int32\n",
      "int32\n",
      "Using device: cpu\n",
      "\n",
      "Training set shape: (126989, 21)\n",
      "Testing set shape: (32201, 21)\n",
      "\n",
      "Training LSTM model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m X_train, X_test, y_train, y_test, label_encoder \u001b[38;5;241m=\u001b[39m load_ravdess_data()\n\u001b[0;32m      5\u001b[0m emotion_detector \u001b[38;5;241m=\u001b[39m EmotionDetectionPyTorch(X_train, X_test, y_train, y_test, label_encoder)\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43memotion_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m      9\u001b[0m emotion_detector\u001b[38;5;241m.\u001b[39msave_model(model)\n\u001b[0;32m     11\u001b[0m emotion_detector\u001b[38;5;241m.\u001b[39mprint_comparison()\n",
      "Cell \u001b[1;32mIn[191], line 204\u001b[0m, in \u001b[0;36mEmotionDetectionPyTorch.run_experiment\u001b[1;34m(self, model_name, batch_size, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    201\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_lstm_model()\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m trained_model, history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m    209\u001b[0m conf_matrix, class_report, pred_emotions, true_emotions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate_model(\n\u001b[0;32m    210\u001b[0m     trained_model, test_loader\n\u001b[0;32m    211\u001b[0m )\n",
      "Cell \u001b[1;32mIn[191], line 88\u001b[0m, in \u001b[0;36mEmotionDetectionPyTorch.train_model\u001b[1;34m(self, model, train_loader, test_loader, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n\u001b[0;32m     87\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 88\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Nini\\Capstone\\src\\capstone_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    491\u001b[0m             )\n\u001b[1;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Nini\\Capstone\\src\\capstone_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Nini\\Capstone\\src\\capstone_env\\Lib\\site-packages\\torch\\optim\\adam.py:218\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;129m@_use_grad_for_differentiable\u001b[39m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    212\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform a single optimization step.\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m        closure (Callable, optional): A closure that reevaluates the model\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m            and returns the loss.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_graph_capture_health_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Nini\\Capstone\\src\\capstone_env\\Lib\\site-packages\\torch\\optim\\optimizer.py:436\u001b[0m, in \u001b[0;36mOptimizer._cuda_graph_capture_health_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_cuda_graph_capture_health_check\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;66;03m# Note [torch.compile x capturable]\u001b[39;00m\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;66;03m# If we are compiling, we try to take the capturable path automatically by\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;66;03m# Thus, when compiling, inductor will determine if cudagraphs\u001b[39;00m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;66;03m# can be enabled based on whether there is input mutation or CPU tensors.\u001b[39;00m\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcompiler\u001b[38;5;241m.\u001b[39mis_compiling()\n\u001b[0;32m    433\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_built()\n\u001b[0;32m    434\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m    435\u001b[0m     ):\n\u001b[1;32m--> 436\u001b[0m         capturing \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_current_stream_capturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m capturing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[0;32m    439\u001b[0m             group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups\n\u001b[0;32m    440\u001b[0m         ):\n\u001b[0;32m    441\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    442\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting CUDA graph capture of step() for an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    444\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but param_groups\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m capturable is False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    445\u001b[0m             )\n",
      "File \u001b[1;32mc:\\Nini\\Capstone\\src\\capstone_env\\Lib\\site-packages\\torch\\cuda\\graphs.py:30\u001b[0m, in \u001b[0;36mis_current_stream_capturing\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mis_current_stream_capturing\u001b[39m():\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return True if CUDA graph capture is underway on the current CUDA stream, False otherwise.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    If a CUDA context does not exist on the current device, returns False without initializing the context.\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cuda_isCurrentStreamCapturing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    X_train, X_test, y_train, y_test, label_encoder = load_ravdess_data()\n",
    "    \n",
    "    emotion_detector = EmotionDetectionPyTorch(X_train, X_test, y_train, y_test, label_encoder)\n",
    "    \n",
    "    model = emotion_detector.run_experiment()  \n",
    "    \n",
    "    emotion_detector.save_model(model)\n",
    "    \n",
    "    emotion_detector.print_comparison()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7022153,
     "sourceId": 11239813,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
