{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=8)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=8)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.conv3 = nn.Conv1d(128, 128, kernel_size=8)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=4, padding=2)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.conv4 = nn.Conv1d(128, 64, kernel_size=3, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.bn4(self.conv4(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EmotionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=192, num_classes=8, dropout_rate=0.3):\n",
    "        super(EmotionLSTM, self).__init__()\n",
    "        \n",
    "        # Simplified architecture - one bidirectional LSTM\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Simple dense layers\n",
    "        self.bn = nn.BatchNorm1d(hidden_size*2)\n",
    "        self.fc1 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM layer\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Get last time step or squeeze\n",
    "        if lstm_out.size(1) == 1:\n",
    "            lstm_out = lstm_out.squeeze(1)\n",
    "        else:\n",
    "            lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply batch normalization\n",
    "        lstm_out = self.bn(lstm_out)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_EMOTIONS = 8  # For RAVDESS dataset: neutral, calm, happy, sad, angry, fearful, disgust, surprised\n",
    "EMOTION_LABELS = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "\n",
    "# Voting Ensemble Model\n",
    "class VotingEnsemble(nn.Module):\n",
    "    def __init__(self, speech_model, facial_model):\n",
    "        super(VotingEnsemble, self).__init__()\n",
    "        self.speech_model = speech_model\n",
    "        self.facial_model = facial_model\n",
    "        \n",
    "        # Freeze base models (optional - you can set to True if you want to keep original models fixed)\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Learnable weights for each model\n",
    "        self.speech_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        self.facial_weight = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        # Additional fusion layer for feature-level combination\n",
    "        self.fusion_layer = nn.Linear(NUM_EMOTIONS * 2, NUM_EMOTIONS)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, speech_input, facial_input):\n",
    "        # Get predictions from individual models\n",
    "        speech_out = self.speech_model(speech_input)\n",
    "        facial_out = self.facial_model(facial_input)\n",
    "        \n",
    "        # Method 1: Weighted average of probabilities\n",
    "        speech_probs = torch.softmax(speech_out, dim=1) * torch.sigmoid(self.speech_weight)\n",
    "        facial_probs = torch.softmax(facial_out, dim=1) * torch.sigmoid(self.facial_weight)\n",
    "        weighted_avg = (speech_probs + facial_probs) / (torch.sigmoid(self.speech_weight) + torch.sigmoid(self.facial_weight))\n",
    "        \n",
    "        # Method 2: Feature-level fusion\n",
    "        # concat_features = torch.cat((speech_out, facial_out), dim=1)\n",
    "        # fused_output = self.fusion_layer(self.dropout(concat_features))\n",
    "        \n",
    "        # You can choose which method to use based on performance\n",
    "        return weighted_avg  # Method 1\n",
    "        # return fused_output  # Method 2\n",
    "\n",
    "class MaxConfidenceEnsemble(nn.Module):\n",
    "    def __init__(self, speech_model, facial_model):\n",
    "        super(MaxConfidenceEnsemble, self).__init__()\n",
    "        self.speech_model = speech_model\n",
    "        self.facial_model = facial_model\n",
    "        \n",
    "        # Freeze models (optional)\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, speech_input, facial_input):\n",
    "    # Get logits\n",
    "        speech_out = self.speech_model(speech_input)\n",
    "        facial_out = self.facial_model(facial_input)\n",
    "\n",
    "        # Get probabilities\n",
    "        speech_probs = torch.softmax(speech_out, dim=1)\n",
    "        facial_probs = torch.softmax(facial_out, dim=1)\n",
    "\n",
    "        # Get max confidence per sample\n",
    "        speech_max_conf, _ = speech_probs.max(dim=1)\n",
    "        facial_max_conf, _ = facial_probs.max(dim=1)\n",
    "\n",
    "        # ðŸ” Debug: Count which model is selected\n",
    "        with torch.no_grad():\n",
    "            speech_selected = (speech_max_conf >= facial_max_conf).sum().item()\n",
    "            facial_selected = (facial_max_conf > speech_max_conf).sum().item()\n",
    "            print(f\"Speech selected: {speech_selected}, Facial selected: {facial_selected}\")\n",
    "\n",
    "        # Choose output with higher or equal confidence for speech\n",
    "        margin = 0.05  # 5% threshold\n",
    "        final_probs = torch.where(\n",
    "            ((facial_max_conf - speech_max_conf) > margin).unsqueeze(1),\n",
    "            facial_probs,\n",
    "            speech_probs\n",
    "        )\n",
    "\n",
    "        return final_probs\n",
    "\n",
    "# Evaluation functions\n",
    "def evaluate_model(model, speech_data, facial_data, labels, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate the ensemble model and return metrics\n",
    "    \n",
    "    Args:\n",
    "        model: The ensemble model\n",
    "        speech_data: Speech features (tensor)\n",
    "        facial_data: Facial features (tensor)\n",
    "        labels: Ground truth labels (tensor)\n",
    "        device: Device to run evaluation on ('cuda' or 'cpu')\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing various metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Move data to device\n",
    "    speech_data = speech_data.to(device)\n",
    "    facial_data = facial_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(speech_data, facial_data)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        # Convert to numpy for sklearn metrics\n",
    "        predicted_np = predicted.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(labels_np, predicted_np)\n",
    "        report = classification_report(labels_np, predicted_np, target_names=EMOTION_LABELS, output_dict=True)\n",
    "        conf_matrix = confusion_matrix(labels_np, predicted_np)\n",
    "        \n",
    "        # Per-class metrics\n",
    "        per_class_acc = report['weighted avg']['precision']\n",
    "        \n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'per_class_accuracy': per_class_acc,\n",
    "            'classification_report': report,\n",
    "            'confusion_matrix': conf_matrix\n",
    "        }\n",
    "\n",
    "def visualize_confusion_matrix(conf_matrix, class_names=EMOTION_LABELS):\n",
    "    \"\"\"\n",
    "    Visualize confusion matrix\n",
    "    \n",
    "    Args:\n",
    "        conf_matrix: Confusion matrix\n",
    "        class_names: List of class names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_individual_vs_ensemble(speech_model, facial_model, ensemble_model, \n",
    "                                   speech_data, facial_data, labels, device='cuda'):\n",
    "    \"\"\"\n",
    "    Compare individual models vs ensemble\n",
    "    \n",
    "    Args:\n",
    "        speech_model: Speech emotion model\n",
    "        facial_model: Facial emotion model\n",
    "        ensemble_model: Ensemble model\n",
    "        speech_data: Speech features (tensor)\n",
    "        facial_data: Facial features (tensor)\n",
    "        labels: Ground truth labels (tensor)\n",
    "        device: Device to run evaluation on ('cuda' or 'cpu')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with all evaluation results\n",
    "    \"\"\"\n",
    "    # Move data to device\n",
    "    speech_data = speech_data.to(device)\n",
    "    facial_data = facial_data.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    speech_model.eval()\n",
    "    facial_model.eval()\n",
    "    ensemble_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Individual model predictions\n",
    "        speech_outputs = speech_model(speech_data)\n",
    "        facial_outputs = facial_model(facial_data)\n",
    "        ensemble_outputs = ensemble_model(speech_data, facial_data)\n",
    "        \n",
    "        # Get predicted classes\n",
    "        _, speech_preds = torch.max(speech_outputs, 1)\n",
    "        _, facial_preds = torch.max(facial_outputs, 1)\n",
    "        _, ensemble_preds = torch.max(ensemble_outputs, 1)\n",
    "        \n",
    "        # Convert to numpy\n",
    "        speech_preds_np = speech_preds.cpu().numpy()\n",
    "        facial_preds_np = facial_preds.cpu().numpy()\n",
    "        ensemble_preds_np = ensemble_preds.cpu().numpy()\n",
    "        labels_np = labels.cpu().numpy()\n",
    "        \n",
    "        # Calculate accuracies\n",
    "        speech_acc = accuracy_score(labels_np, speech_preds_np)\n",
    "        facial_acc = accuracy_score(labels_np, facial_preds_np)\n",
    "        ensemble_acc = accuracy_score(labels_np, ensemble_preds_np)\n",
    "        \n",
    "        # Generate classification reports\n",
    "        speech_report = classification_report(labels_np, speech_preds_np, \n",
    "                                             target_names=EMOTION_LABELS, output_dict=True)\n",
    "        facial_report = classification_report(labels_np, facial_preds_np, \n",
    "                                             target_names=EMOTION_LABELS, output_dict=True)\n",
    "        ensemble_report = classification_report(labels_np, ensemble_preds_np, \n",
    "                                              target_names=EMOTION_LABELS, output_dict=True)\n",
    "        \n",
    "        # Generate confusion matrices\n",
    "        speech_cm = confusion_matrix(labels_np, speech_preds_np)\n",
    "        facial_cm = confusion_matrix(labels_np, facial_preds_np)\n",
    "        ensemble_cm = confusion_matrix(labels_np, ensemble_preds_np)\n",
    "        \n",
    "        # Return all results\n",
    "        return {\n",
    "            'speech': {\n",
    "                'accuracy': speech_acc,\n",
    "                'classification_report': speech_report,\n",
    "                'confusion_matrix': speech_cm\n",
    "            },\n",
    "            'facial': {\n",
    "                'accuracy': facial_acc,\n",
    "                'classification_report': facial_report,\n",
    "                'confusion_matrix': facial_cm\n",
    "            },\n",
    "            'ensemble': {\n",
    "                'accuracy': ensemble_acc,\n",
    "                'classification_report': ensemble_report,\n",
    "                'confusion_matrix': ensemble_cm\n",
    "            }\n",
    "        }\n",
    "\n",
    "def plot_comparison(results):\n",
    "    \"\"\"\n",
    "    Plot comparison of models\n",
    "    \n",
    "    Args:\n",
    "        results: Results dictionary from evaluate_individual_vs_ensemble\n",
    "    \"\"\"\n",
    "    models = ['Speech Model', 'Facial Model', 'Ensemble Model']\n",
    "    accuracies = [\n",
    "        results['speech']['accuracy'] * 100,\n",
    "        results['facial']['accuracy'] * 100,\n",
    "        results['ensemble']['accuracy'] * 100\n",
    "    ]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(models, accuracies, color=['blue', 'green', 'red'])\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                f'{height:.2f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Model Comparison')\n",
    "    plt.ylim(0, 100)  # Set y-axis from 0 to 100\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_comparison.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def aggregate_facial_features(facial_data, video_ids):\n",
    "    \"\"\"\n",
    "    Aggregate facial features per video ID\n",
    "    \n",
    "    Args:\n",
    "        facial_data: Tensor of facial features\n",
    "        video_ids: List of video IDs corresponding to each facial feature\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (aggregated_features, unique_video_ids)\n",
    "    \"\"\"\n",
    "    video_to_frames = {}\n",
    "    \n",
    "    # Group frame indices by video ID\n",
    "    for i, vid in enumerate(video_ids):\n",
    "        if vid not in video_to_frames:\n",
    "            video_to_frames[vid] = []\n",
    "        video_to_frames[vid].append(i)\n",
    "    \n",
    "    # Aggregate frames for each video\n",
    "    aggregated_features = []\n",
    "    unique_video_ids = []\n",
    "    \n",
    "    for vid, frame_indices in video_to_frames.items():\n",
    "        video_frames = facial_data[frame_indices]\n",
    "        # Average across frames\n",
    "        avg_features = torch.mean(video_frames, dim=0, keepdim=True)\n",
    "        aggregated_features.append(avg_features)\n",
    "        unique_video_ids.append(vid)\n",
    "    \n",
    "    if len(aggregated_features) > 0:\n",
    "        return torch.cat(aggregated_features, dim=0), unique_video_ids\n",
    "    else:\n",
    "        return None, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original speech data shape: torch.Size([565, 1, 128])\n",
      "Original facial data shape: torch.Size([5681, 1, 21])\n",
      "Original labels shape: torch.Size([565])\n",
      "Aggregated facial data shape: torch.Size([52, 1, 21])\n",
      "Number of unique facial videos: 52\n",
      "Found 52 common videos between datasets\n",
      "Aligned speech data shape: torch.Size([52, 1, 128])\n",
      "Aligned facial data shape: torch.Size([52, 1, 21])\n",
      "Aligned labels shape: torch.Size([52])\n",
      "Speech\n",
      "tensor([[[ 0.7161,  0.9442,  0.6214,  ...,  0.6145,  0.7294,  0.9747]],\n",
      "\n",
      "        [[ 0.2393,  0.2322,  1.0326,  ..., -1.0124, -1.0052, -0.9658]],\n",
      "\n",
      "        [[ 0.5010,  0.5907, -0.0187,  ..., -0.6323, -0.8343, -0.9241]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 2.0672,  1.7239,  1.4174,  ...,  1.8156,  1.7393,  1.7581]],\n",
      "\n",
      "        [[ 1.2390,  0.8291,  0.9826,  ...,  0.0247,  0.0819,  0.1359]],\n",
      "\n",
      "        [[-0.0484,  0.3097,  0.4756,  ...,  0.3339,  0.3580,  0.4293]]])\n",
      "tensor([[[3.3740e-01, 1.5618e-01, 3.4763e-01,  ..., 2.0077e+00,\n",
      "          1.2318e+02, 3.0090e+03]],\n",
      "\n",
      "        [[3.5752e-01, 1.5290e-01, 3.3410e-01,  ..., 2.0114e+00,\n",
      "          1.2140e+02, 2.8729e+03]],\n",
      "\n",
      "        [[3.4256e-01, 2.0433e-01, 3.3851e-01,  ..., 2.0112e+00,\n",
      "          1.2538e+02, 3.7982e+03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[3.4001e-01, 2.8034e-01, 3.1309e-01,  ..., 1.9038e+00,\n",
      "          1.3010e+02, 4.2531e+03]],\n",
      "\n",
      "        [[3.2974e-01, 2.7288e-01, 3.5269e-01,  ..., 2.0114e+00,\n",
      "          1.1181e+02, 4.4161e+03]],\n",
      "\n",
      "        [[3.4134e-01, 2.1827e-01, 3.0915e-01,  ..., 1.9377e+00,\n",
      "          1.3601e+02, 3.4488e+03]]])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Load your models and data here\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # For CSV file\n",
    "    # Read the CSV file\n",
    "    df_csv = pd.read_csv(\"C:/Nini/Capstone/CSV_Files/Speech data/Speech_Test_preprocessed_new.csv\")\n",
    "    # Convert to a PyTorch tensor\n",
    "    X_train_speech = df_csv.drop(['Unnamed: 0','Path','Emotion'], axis=1).to_numpy()  \n",
    "    csv_tensor = torch.tensor(X_train_speech, dtype=torch.float32).unsqueeze(1)\n",
    "    # Save the tensor to a .pt file\n",
    "    torch.save(csv_tensor, 'C:/Nini/Capstone/CSV_Files/Speech data/speech_test_data.pt')\n",
    "    \n",
    "    # For Excel file\n",
    "    # Read the Excel file\n",
    "    df_excel = pd.read_excel(\"C:/Nini/Capstone/CSV_Files/Facial data/FacialFeatures_Test.xlsx\")\n",
    "    X_train_face = df_excel.drop(['Unnamed: 0','emotion','video_name','BaseName'], axis=1).to_numpy()  \n",
    "    # Convert to a PyTorch tensor\n",
    "    excel_tensor = torch.tensor(X_train_face, dtype=torch.float32).unsqueeze(1)\n",
    "    # Save the tensor to a .pt file\n",
    "    torch.save(excel_tensor, 'C:/Nini/Capstone/CSV_Files/Facial data/face_test_data.pt')\n",
    "\n",
    "    # Load the labels from the pickle file\n",
    "    lb = joblib.load(\"C:/Nini/Capstone/src/Model_training/label_encoder.pkl\")\n",
    "\n",
    "    raw_labels = df_csv['Emotion']\n",
    "    \n",
    "    # Transform using your label encoder\n",
    "    encoded_labels = lb.transform(raw_labels)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    test_labels_tensor = torch.tensor(encoded_labels)\n",
    "\n",
    "    # If you want to save as a .pt file for future use\n",
    "    torch.save(test_labels_tensor, 'C:/Nini/Capstone/CSV_Files/Speech data/test_labels.pt')\n",
    "    \n",
    "    # Load test data\n",
    "    speech_test_data = torch.load('C:/Nini/Capstone/CSV_Files/Speech data/speech_test_data.pt')\n",
    "    facial_test_data = torch.load('C:/Nini/Capstone/CSV_Files/Facial data/face_test_data.pt')\n",
    "    test_labels = torch.load('C:/Nini/Capstone/CSV_Files/Speech data/test_labels.pt')\n",
    "\n",
    "    # Print original data shapes\n",
    "    print(f\"Original speech data shape: {speech_test_data.shape}\")\n",
    "    print(f\"Original facial data shape: {facial_test_data.shape}\")\n",
    "    print(f\"Original labels shape: {test_labels.shape}\")\n",
    "\n",
    "    # Get video IDs from both datasets\n",
    "    speech_video_names = df_csv['Path'].values\n",
    "    speech_video_ids = [os.path.splitext(os.path.basename(f))[0] for f in speech_video_names]\n",
    "    facial_video_names = df_excel['video_name'].values\n",
    "    facial_video_ids = [os.path.splitext(os.path.basename(f))[0] for f in facial_video_names]\n",
    "    \n",
    "    # Aggregate facial features per video\n",
    "    aggregated_facial_data, unique_facial_video_ids = aggregate_facial_features(\n",
    "        facial_test_data, facial_video_ids\n",
    "    )\n",
    "    \n",
    "    print(f\"Aggregated facial data shape: {aggregated_facial_data.shape}\")\n",
    "    print(f\"Number of unique facial videos: {len(unique_facial_video_ids)}\")\n",
    "    \n",
    "    # Find common videos between datasets\n",
    "    common_video_ids = set(speech_video_ids).intersection(set(unique_facial_video_ids))\n",
    "    print(f\"Found {len(common_video_ids)} common videos between datasets\")\n",
    "    \n",
    "    if len(common_video_ids) == 0:\n",
    "        print(\"ERROR: No common videos found between datasets!\")\n",
    "        # As a fallback, ensure tensors have the same first dimension\n",
    "        min_samples = min(speech_test_data.shape[0], aggregated_facial_data.shape[0])\n",
    "        aligned_speech_data = speech_test_data[:min_samples]\n",
    "        aligned_facial_data = aggregated_facial_data[:min_samples]\n",
    "        aligned_labels = test_labels[:min_samples]\n",
    "        print(f\"Using first {min_samples} samples from each dataset as fallback\")\n",
    "    else:\n",
    "        # Create indices for the common videos\n",
    "        speech_indices = [i for i, vid in enumerate(speech_video_ids) if vid in common_video_ids]\n",
    "        facial_indices = [i for i, vid in enumerate(unique_facial_video_ids) if vid in common_video_ids]\n",
    "        \n",
    "        # Ensure we have the same order of videos in both datasets\n",
    "        speech_video_order = [speech_video_ids[i] for i in speech_indices]\n",
    "        facial_video_order = [unique_facial_video_ids[i] for i in facial_indices]\n",
    "        \n",
    "        # Create a mapping to reorder facial indices if needed\n",
    "        reordering = []\n",
    "        for vid in speech_video_order:\n",
    "            if vid in facial_video_order:\n",
    "                reordering.append(facial_video_order.index(vid))\n",
    "        \n",
    "        # Extract the aligned data\n",
    "        aligned_speech_data = speech_test_data[speech_indices]\n",
    "        aligned_facial_data = aggregated_facial_data[reordering]\n",
    "        aligned_labels = test_labels[speech_indices]\n",
    "    \n",
    "    print(f\"Aligned speech data shape: {aligned_speech_data.shape}\")\n",
    "    print(f\"Aligned facial data shape: {aligned_facial_data.shape}\")\n",
    "    print(f\"Aligned labels shape: {aligned_labels.shape}\")\n",
    "\n",
    "    print(\"Speech\")\n",
    "    print(aligned_speech_data)\n",
    "    print(aligned_facial_data)\n",
    "    # Initialize your models\n",
    "    speech_model = CNNModel(input_size=X_train_speech.shape[1], num_classes=8).to(device)\n",
    "    facial_model = EmotionLSTM(input_size=X_train_face.shape[1]).to(device)\n",
    "    \n",
    "    # Load pre-trained weights\n",
    "    speech_model.load_state_dict(torch.load('C:/Nini/Capstone/Models/DataAugmentation_cnn_model_new_final.pth', weights_only=True))\n",
    "    facial_model.load_state_dict(torch.load('C:/Nini/Capstone/src/Facial Models/emotion_lstm_model.pth', weights_only=True))\n",
    "    \n",
    "    # Create ensemble model\n",
    "    # ensemble_model = VotingEnsemble(speech_model, facial_model).to(device)\n",
    "    ensemble_model = MaxConfidenceEnsemble(speech_model, facial_model).to(device)\n",
    "    \n",
    "    # Evaluate models with aligned data\n",
    "    results = evaluate_individual_vs_ensemble(\n",
    "        speech_model, facial_model, ensemble_model,\n",
    "        aligned_speech_data, aligned_facial_data, aligned_labels, device\n",
    "    )\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Speech Model Accuracy: {results['speech']['accuracy']:.4f}\")\n",
    "    print(f\"Facial Model Accuracy: {results['facial']['accuracy']:.4f}\")\n",
    "    print(f\"Ensemble Model Accuracy: {results['ensemble']['accuracy']:.4f}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    plot_comparison(results)\n",
    "    \n",
    "    # Visualize confusion matrix for the ensemble\n",
    "    visualize_confusion_matrix(results['ensemble']['confusion_matrix'])\n",
    "    \n",
    "    # Print detailed report for ensemble\n",
    "    print(\"\\nEnsemble Classification Report:\")\n",
    "    for emotion, metrics in results['ensemble']['classification_report'].items():\n",
    "        if isinstance(metrics, dict):\n",
    "            print(f\"{emotion}: precision={metrics['precision']:.2f}, recall={metrics['recall']:.2f}, f1-score={metrics['f1-score']:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7125212,
     "sourceId": 11379962,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7128939,
     "sourceId": 11384975,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 300955,
     "modelInstanceId": 280042,
     "sourceId": 334499,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 300957,
     "modelInstanceId": 280044,
     "sourceId": 334501,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
