{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338cc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082cc06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        # First convolutional block\n",
    "        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_1 = nn.BatchNorm2d(32)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Second convolutional block\n",
    "        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_1 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Third convolutional block\n",
    "        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_1 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        \n",
    "        # Calculate the flattened size dynamically based on input size 48x48\n",
    "        self.flatten_size = 128 * (48 // 8) * (48 // 8)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 512)\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.dropout_fc1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(256)\n",
    "        self.dropout_fc2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First block\n",
    "        x = F.relu(self.bn1_1(self.conv1_1(x)))\n",
    "        x = F.relu(self.bn1_2(self.conv1_2(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Second block\n",
    "        x = F.relu(self.bn2_1(self.conv2_1(x)))\n",
    "        x = F.relu(self.bn2_2(self.conv2_2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Third block\n",
    "        x = F.relu(self.bn3_1(self.conv3_1(x)))\n",
    "        x = F.relu(self.bn3_2(self.conv3_2(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout_fc1(x)\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x)))\n",
    "        x = self.dropout_fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d1ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, features, labels, transform=None):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Assuming features are already preprocessed as normalized grayscale images\n",
    "        # and have shape (H, W) so we need to add channel dimension\n",
    "        image = self.features[idx]\n",
    "        \n",
    "        # Ensure image has shape (1, H, W) for PyTorch conv layers\n",
    "        if len(image.shape) == 2:\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            \n",
    "        image = torch.FloatTensor(image)\n",
    "        \n",
    "        label = self.labels[idx]\n",
    "        if isinstance(label, np.ndarray):\n",
    "            # Convert one-hot to class index if needed\n",
    "            label = np.argmax(label) \n",
    "        \n",
    "        label = torch.LongTensor([label])[0]  # Convert to PyTorch tensor\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ea795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device='cuda'):\n",
    "    # Move model to device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Lists to store metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Best validation accuracy for model saving\n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = running_loss / total\n",
    "        epoch_train_acc = correct / total\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        train_accs.append(epoch_train_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_val_loss = running_loss / total\n",
    "        epoch_val_acc = correct / total\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, '\n",
    "              f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}')\n",
    "        \n",
    "        # Save the best model\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save(model.state_dict(), 'best_emotion_model.pth')\n",
    "            print(f'Model saved with validation accuracy: {best_val_acc:.4f}')\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_accs, label='Training Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Load best model for evaluation\n",
    "    model.load_state_dict(torch.load('best_emotion_model.pth'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80450455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device='cuda', emotion_map=None):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "    # Create classification report\n",
    "    if emotion_map:\n",
    "        target_names = list(emotion_map.values())\n",
    "    else:\n",
    "        target_names = [f'Class {i}' for i in range(len(set(all_labels)))]\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2180a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(model, image, device='cuda', emotion_map=None):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure image has proper dimensions: (1, 1, H, W)\n",
    "    if len(image.shape) == 2:  # (H, W)\n",
    "        image = np.expand_dims(image, axis=0)  # Add channel dim: (1, H, W)\n",
    "    if len(image.shape) == 3 and image.shape[0] == 1:  # (1, H, W)\n",
    "        image = np.expand_dims(image, axis=0)  # Add batch dim: (1, 1, H, W)\n",
    "    \n",
    "    # Convert to tensor\n",
    "    image_tensor = torch.FloatTensor(image).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        prob_np = probabilities.cpu().numpy()[0]\n",
    "        pred_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    if emotion_map:\n",
    "        emotion = list(emotion_map.values())[pred_class]\n",
    "        return emotion, prob_np[pred_class]\n",
    "    else:\n",
    "        return pred_class, prob_np[pred_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define emotion classes in RAVDESS\n",
    "    emotion_map = {\n",
    "        0: 'neutral',\n",
    "        1: 'calm',\n",
    "        2: 'happy',\n",
    "        3: 'sad',\n",
    "        4: 'angry',\n",
    "        5: 'fearful',\n",
    "        6: 'disgust',\n",
    "        7: 'surprised'\n",
    "    }\n",
    "    \n",
    "    # Hyperparameters\n",
    "    batch_size = 64\n",
    "    num_epochs = 30\n",
    "    learning_rate = 0.0001\n",
    "    num_classes = 8  # RAVDESS has 8 emotions\n",
    "    \n",
    "    # Load your preprocessed data here\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_preprocessed_data()\n",
    "    \n",
    "    # For demonstration, let's assume your preprocessed data is already loaded:\n",
    "    # Replace these with your actual preprocessed data\n",
    "    # X_train shape: (num_samples, 1, 48, 48) or (num_samples, 48, 48)\n",
    "    # y_train shape: (num_samples,) or (num_samples, num_classes) for one-hot\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = EmotionDataset(X_train, y_train)\n",
    "    val_dataset = EmotionDataset(X_val, y_val)\n",
    "    test_dataset = EmotionDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EmotionCNN(num_classes=num_classes)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Train model\n",
    "    model = train_model(model, train_loader, val_loader, criterion, optimizer, \n",
    "                        num_epochs=num_epochs, device=device)\n",
    "    \n",
    "    # Evaluate model\n",
    "    accuracy, preds, labels = evaluate_model(model, test_loader, device=device, \n",
    "                                             emotion_map=emotion_map)\n",
    "    \n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    \n",
    "    # Total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
