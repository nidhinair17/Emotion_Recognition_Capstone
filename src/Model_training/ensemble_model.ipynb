{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    def __init__(self, facial_model, speech_model, num_classes=8, fusion_method='weighted'):\n",
    "        super(EnsembleModel, self).__init__()\n",
    "        \n",
    "        # Load pre-trained models\n",
    "        self.facial_model = facial_model\n",
    "        self.speech_model = speech_model\n",
    "        \n",
    "        # Freeze the base models (optional)\n",
    "        for param in self.facial_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.speech_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Fusion method\n",
    "        self.fusion_method = fusion_method\n",
    "        \n",
    "        if fusion_method == 'concat':\n",
    "            # Input size will be sum of the output sizes of both models\n",
    "            # Assuming facial_model outputs 8 classes and speech_model outputs 8 classes\n",
    "            self.fusion_layer = nn.Linear(num_classes * 2, num_classes)\n",
    "        elif fusion_method == 'weighted':\n",
    "            # Learn weights for each modality\n",
    "            self.alpha = nn.Parameter(torch.tensor([0.5]), requires_grad=True)\n",
    "        elif fusion_method == 'attention':\n",
    "            # Attention-based fusion\n",
    "            self.attention = nn.Sequential(\n",
    "                nn.Linear(num_classes * 2, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 2),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "    def forward(self, facial_input, speech_input):\n",
    "        # Get predictions from individual models\n",
    "        facial_output = self.facial_model(facial_input)\n",
    "        speech_output = self.speech_model(speech_input)\n",
    "        \n",
    "        # Apply fusion method\n",
    "        if self.fusion_method == 'concat':\n",
    "            # Concatenate outputs and pass through fusion layer\n",
    "            combined = torch.cat((facial_output, speech_output), dim=1)\n",
    "            return self.fusion_layer(combined)\n",
    "        \n",
    "        elif self.fusion_method == 'weighted':\n",
    "            # Apply learned weights\n",
    "            alpha = torch.sigmoid(self.alpha)  # Constrain between 0 and 1\n",
    "            return alpha * facial_output + (1 - alpha) * speech_output\n",
    "        \n",
    "        elif self.fusion_method == 'attention':\n",
    "            # Concatenate for attention computation\n",
    "            combined = torch.cat((facial_output, speech_output), dim=1)\n",
    "            weights = self.attention(combined)\n",
    "            \n",
    "            # Apply attention weights\n",
    "            weighted_facial = weights[:, 0].unsqueeze(1) * facial_output\n",
    "            weighted_speech = weights[:, 1].unsqueeze(1) * speech_output\n",
    "            \n",
    "            return weighted_facial + weighted_speech\n",
    "        \n",
    "        elif self.fusion_method == 'max':\n",
    "            # Take maximum confidence for each class\n",
    "            return torch.max(facial_output, speech_output)\n",
    "        \n",
    "        else:  # Default to average\n",
    "            return (facial_output + speech_output) / 2\n",
    "\n",
    "# Function to train the ensemble model\n",
    "def train_ensemble(ensemble_model, train_loader, val_loader, criterion, optimizer, \n",
    "                   device, num_epochs=50, early_stopping_patience=10):\n",
    "    \"\"\"\n",
    "    Train the ensemble model\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ensemble_model : EnsembleModel\n",
    "        The ensemble model to train\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for validation data\n",
    "    criterion : loss function\n",
    "    optimizer : optimizer\n",
    "    device : torch.device\n",
    "        Device to train on (cuda/cpu)\n",
    "    num_epochs : int\n",
    "        Number of epochs to train\n",
    "    early_stopping_patience : int\n",
    "        Number of epochs to wait for improvement before stopping\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [],\n",
    "        'precision': [], 'recall': [], 'f1': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        ensemble_model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for facial_batch, speech_batch, labels in train_loader:\n",
    "            facial_batch, speech_batch, labels = facial_batch.to(device), speech_batch.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = ensemble_model(facial_batch, speech_batch)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_acc = correct / total\n",
    "        \n",
    "        # Validation phase\n",
    "        ensemble_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for facial_batch, speech_batch, labels in val_loader:\n",
    "                facial_batch, speech_batch, labels = facial_batch.to(device), speech_batch.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = ensemble_model(facial_batch, speech_batch)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            all_labels, all_preds, average='weighted', zero_division=1\n",
    "        )\n",
    "        \n",
    "        # Save history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['precision'].append(precision)\n",
    "        history['recall'].append(recall)\n",
    "        history['f1'].append(f1)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(ensemble_model.state_dict(), 'best_ensemble_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Example usage\n",
    "def create_ensemble_and_train(facial_model_path, speech_model_path, train_loader, val_loader):\n",
    "    # Load pre-trained models\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the facial emotion recognition model\n",
    "    facial_model = CNNModel(input_size=YOUR_FACIAL_INPUT_SIZE, num_classes=8).to(device)\n",
    "    facial_model.load_state_dict(torch.load(facial_model_path))\n",
    "    \n",
    "    # Load the speech emotion recognition model \n",
    "    speech_model = CNNModel(input_size=YOUR_SPEECH_INPUT_SIZE, num_classes=8).to(device)\n",
    "    speech_model.load_state_dict(torch.load(speech_model_path))\n",
    "    \n",
    "    # Create ensemble model\n",
    "    ensemble_model = EnsembleModel(facial_model, speech_model, num_classes=8, fusion_method='attention').to(device)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Only optimize the fusion parameters, not the base models\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, ensemble_model.parameters()),\n",
    "        lr=0.001, \n",
    "        weight_decay=1e-5\n",
    "    )\n",
    "    \n",
    "    # Train the ensemble\n",
    "    history = train_ensemble(\n",
    "        ensemble_model, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        criterion, \n",
    "        optimizer, \n",
    "        device,\n",
    "        num_epochs=100,\n",
    "        early_stopping_patience=15\n",
    "    )\n",
    "    \n",
    "    # Plot results\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history['precision'], label='Precision')\n",
    "    plt.plot(history['recall'], label='Recall')\n",
    "    plt.plot(history['f1'], label='F1-Score')\n",
    "    plt.title('Model Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "# Data preprocessing and model evaluation utilities\n",
    "class MultiModalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, facial_features, speech_features, labels):\n",
    "        \"\"\"\n",
    "        Dataset for multimodal input (facial and speech)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        facial_features : array-like\n",
    "            Features from facial expressions\n",
    "        speech_features : array-like\n",
    "            Features from speech audio\n",
    "        labels : array-like\n",
    "            Emotion labels\n",
    "        \"\"\"\n",
    "        self.facial_features = torch.FloatTensor(facial_features)\n",
    "        self.speech_features = torch.FloatTensor(speech_features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.facial_features[idx], self.speech_features[idx], self.labels[idx]\n",
    "    \n",
    "def evaluate_ensemble(model, test_loader, device):\n",
    "    \"\"\"Evaluate the ensemble model on test data\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for facial_batch, speech_batch, labels in test_loader:\n",
    "            facial_batch = facial_batch.to(device)\n",
    "            speech_batch = speech_batch.to(device)\n",
    "            \n",
    "            outputs = model(facial_batch, speech_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='weighted', zero_division=1\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',\n",
    "                xticklabels=range(8), yticklabels=range(8))\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, precision, recall, f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
